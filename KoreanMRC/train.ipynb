{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get install -y g++ openjdk-8-jdk python3-dev curl git build-essential\n",
    "! pip install konlpy \"tweepy<4.0.0\"\n",
    "! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ko_mrc.datasets as datasets\n",
    "import ko_mrc.utils as utils\n",
    "import ko_mrc.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Questions 12037\n",
      "{'guid': '798db07f0b9046759deed9d4a35ce31e', 'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.', 'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?', 'answers': [{'text': '한 달가량', 'answer_start': 478}, {'text': '한 달', 'answer_start': 478}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.KoMRC.load('data/train.json')\n",
    "print(\"Number of Questions\", len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sentencepiece (subword tokenizer) add - Jinha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-macosx_10_6_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 5.0 MB/s            \n",
      "\u001B[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Questions 10834 1203\n",
      "{'guid': '531b9879ffb54f64964b47fd832aa3af', 'context_original': '◇새로운 디지털 시대=세계적 정보기술(IT) 기업 구글의 에릭 슈밋 회장과 구글의 싱크탱크인 구글 아이디어의 제러드 코언 소장이 쓴 IT 시대 미래 예측서. 세계인이 온라인 세상에서 만나게 될 모습을 다뤘다. 지난해 초판 발행 이후 데이터 영구화, 중산층 일자리, 자유와 안보, 디지털 혁명 문제 등을 추가한 개정 증보판이다.(이진원 옮김, 알키, 520쪽, 2만원)◇종횡무진 역사=동양과 서양 문명을 비교하며 두 문명의 탄생, 만남, 융합을 그린 책. 과거 문명 탐구부터 문명의 발전 방향까지 살폈다. 책 이름에 맞게 일방통행적 역사 서술이 아니라 시·공간을 넘나들며 역사 현장을 누빈다. 역사를 현실과 함께 이해하려는 저자의 문제의식이 돋보인다.(남경태 지음, 휴머니스트, 744쪽, 3만8000원)◇영원의 철학=‘영원의 철학’은 16세기에 처음 언급된 ‘모든 위대한 종교의 본질적이고 공통된 핵심 진리’로, 세계의 종교적 전통이 공유하는 세계관, 인간관, 윤리관을 말한다.《멋진 신세계》의 올더스 헉슬리가 420개의 동서고금 문헌을 통해 영원의 철학을 소개한다. 인용문만 읽어도 흥미로운 종교서이자 명상서.(조옥경 옮김, 김영사, 528쪽, 1만9800원)◇대한민국 5천년 역사리더십을 말한다=고조선의 단군부터 이명박 전 대통령에 이르는 우리 민족의 리더십을 다룬 책. 리더십의 관점에서 우리 역사를 바라보며 역사 속 사건들에서 리더십이 어떤 힘을 발휘했는지 살펴보고 홍익인간, 민족주의, 민주주의 등 우리 민족 고유의 리더십 DNA 여덟 가지를 제시한다.(최익용 지음, 옥당, 436쪽, 2만2000원)◇한국 자본주의 모델=진보적 한국 경제론의 이론과 참여를 주도해 온 저자가 광복 이후 박근혜 정부까지 70여년의 대한민국 경제·자본주의 모델을 분석한 책. 이승만 정부의 대미 의존 경제, 박정희 개발주의의 명암을 살펴보면서 민주화 이후의 한국 자본주의와 외환위기 후 경제민주화와 한국 경제의 전망을 탐색한다.(이병천 지음, 책세상, 480쪽, 2만5000원)', 'context_position': [(0, 1), (1, 4), (5, 8), (9, 11), (11, 12), (12, 14), (14, 15), (16, 18), (18, 20), (20, 21), (21, 23), (23, 24), (25, 27), (28, 30), (30, 31), (32, 34), (35, 37), (38, 40), (40, 41), (42, 44), (44, 45), (46, 50), (50, 51), (52, 54), (55, 59), (59, 60), (61, 64), (65, 67), (68, 70), (70, 71), (72, 73), (74, 76), (77, 79), (80, 82), (83, 85), (85, 86), (86, 87), (88, 91), (91, 92), (93, 96), (97, 99), (99, 101), (102, 104), (104, 105), (106, 107), (108, 110), (110, 111), (112, 114), (114, 115), (115, 116), (117, 120), (121, 123), (124, 126), (127, 129), (130, 133), (134, 137), (137, 138), (139, 141), (141, 142), (143, 146), (146, 147), (148, 150), (150, 151), (152, 154), (154, 155), (156, 159), (160, 162), (163, 165), (166, 167), (167, 168), (169, 171), (171, 172), (173, 175), (176, 179), (179, 180), (180, 181), (181, 182), (182, 183), (183, 186), (187, 189), (189, 190), (191, 193), (193, 194), (195, 198), (198, 199), (199, 200), (201, 202), (202, 203), (203, 204), (204, 205), (205, 206), (206, 208), (208, 210), (211, 213), (213, 214), (214, 216), (216, 217), (218, 220), (221, 223), (223, 224), (225, 227), (227, 228), (228, 229), (230, 231), (232, 234), (234, 235), (236, 238), (238, 239), (240, 242), (242, 243), (244, 246), (246, 247), (248, 250), (251, 252), (252, 253), (254, 256), (257, 259), (260, 262), (262, 264), (265, 267), (267, 268), (269, 271), (272, 274), (274, 276), (277, 279), (279, 280), (280, 281), (282, 283), (284, 286), (286, 287), (288, 289), (289, 290), (291, 295), (295, 296), (297, 299), (300, 302), (302, 303), (304, 306), (306, 307), (308, 309), (309, 310), (310, 312), (312, 313), (314, 317), (317, 318), (319, 321), (322, 324), (324, 325), (326, 329), (329, 330), (331, 333), (333, 334), (335, 337), (337, 338), (339, 341), (342, 344), (344, 345), (345, 347), (348, 350), (350, 351), (352, 354), (354, 356), (356, 357), (358, 362), (362, 363), (363, 364), (364, 367), (368, 370), (370, 371), (372, 377), (377, 378), (379, 382), (382, 383), (383, 384), (385, 386), (386, 387), (387, 391), (391, 392), (392, 393), (393, 394), (394, 396), (396, 397), (398, 400), (400, 402), (402, 404), (404, 405), (406, 408), (408, 409), (409, 410), (411, 413), (413, 415), (415, 416), (417, 419), (420, 422), (422, 423), (424, 425), (425, 427), (428, 430), (430, 431), (432, 434), (434, 435), (436, 438), (438, 439), (439, 440), (440, 441), (442, 444), (444, 445), (446, 448), (449, 451), (451, 452), (452, 453), (453, 454), (455, 457), (457, 458), (459, 461), (461, 462), (463, 465), (465, 466), (467, 469), (469, 470), (470, 471), (472, 475), (475, 476), (477, 480), (480, 481), (482, 485), (485, 486), (487, 488), (488, 490), (490, 491), (491, 492), (492, 494), (495, 498), (498, 499), (499, 500), (501, 504), (505, 508), (508, 509), (510, 513), (513, 514), (514, 515), (516, 520), (521, 523), (523, 524), (525, 527), (528, 530), (530, 531), (532, 534), (534, 535), (536, 538), (538, 540), (540, 541), (542, 545), (545, 546), (547, 548), (548, 550), (551, 553), (553, 555), (556, 558), (558, 559), (559, 561), (562, 564), (564, 565), (565, 566), (566, 567), (567, 569), (569, 570), (571, 573), (573, 574), (575, 578), (578, 579), (580, 583), (583, 584), (584, 585), (586, 587), (587, 588), (588, 592), (592, 593), (593, 594), (594, 595), (595, 599), (600, 601), (601, 602), (602, 603), (604, 606), (606, 609), (609, 610), (611, 612), (612, 614), (614, 615), (615, 616), (616, 618), (618, 619), (620, 622), (622, 624), (625, 628), (629, 630), (631, 634), (634, 635), (636, 638), (638, 639), (640, 642), (643, 645), (645, 646), (647, 650), (650, 651), (652, 654), (655, 656), (656, 657), (658, 661), (661, 662), (663, 665), (665, 667), (668, 670), (671, 673), (673, 674), (675, 678), (678, 679), (680, 682), (683, 684), (685, 687), (687, 688), (688, 690), (691, 694), (694, 695), (696, 698), (699, 700), (700, 701), (702, 704), (704, 705), (705, 707), (708, 711), (711, 712), (713, 717), (717, 718), (719, 723), (723, 724), (725, 729), (730, 731), (732, 734), (735, 737), (738, 740), (740, 741), (742, 745), (746, 749), (750, 752), (753, 755), (755, 756), (757, 759), (759, 761), (761, 762), (762, 763), (763, 766), (767, 769), (769, 770), (771, 773), (773, 774), (775, 778), (778, 779), (779, 780), (781, 782), (782, 783), (783, 787), (787, 788), (788, 789), (789, 790), (790, 792), (793, 797), (798, 800), (800, 801), (801, 803), (803, 804), (805, 807), (808, 810), (810, 811), (811, 812), (813, 815), (815, 816), (817, 819), (819, 820), (821, 823), (823, 824), (825, 826), (827, 829), (829, 830), (831, 833), (834, 836), (837, 840), (841, 843), (843, 845), (846, 848), (848, 849), (849, 850), (850, 851), (852, 856), (857, 859), (859, 860), (860, 864), (865, 867), (867, 868), (869, 871), (871, 872), (873, 874), (874, 875), (876, 879), (880, 882), (882, 883), (884, 886), (887, 889), (890, 892), (892, 893), (894, 897), (898, 902), (902, 903), (904, 906), (906, 907), (908, 911), (911, 913), (914, 916), (916, 917), (918, 920), (920, 921), (922, 924), (925, 929), (929, 930), (931, 933), (933, 935), (936, 937), (938, 940), (940, 942), (942, 943), (943, 944), (945, 947), (948, 950), (950, 951), (952, 954), (954, 955), (956, 958), (958, 960), (960, 961), (961, 962), (962, 965), (966, 968), (968, 969), (970, 971), (971, 973), (973, 974), (975, 978), (978, 979), (979, 980), (981, 982), (982, 983), (983, 987), (987, 988), (988, 989)], 'question_original': '영원의 철학에 대한 소개의 글을 쓰기 위해 참조한 문헌의 수는?', 'context': ['◇', '새로운', '디지털', '시대', '=', '세계', '적', '정보', '기술', '(', 'IT', ')', '기업', '구글', '의', '에릭', '슈밋', '회장', '과', '구글', '의', '싱크탱크', '인', '구글', '아이디어', '의', '제러드', '코언', '소장', '이', '쓴', 'IT', '시대', '미래', '예측', '서', '.', '세계인', '이', '온라인', '세상', '에서', '만나', '게', '될', '모습', '을', '다뤘', '다', '.', '지난해', '초판', '발행', '이후', '데이터', '영구화', ',', '중산', '층', '일자리', ',', '자유', '와', '안보', ',', '디지털', '혁명', '문제', '등', '을', '추가', '한', '개정', '증보판', '이', '다', '.', '(', '이진원', '옮김', ',', '알키', ',', '520', '쪽', ',', '2', '만', '원', ')', '◇', '종횡', '무진', '역사', '=', '동양', '과', '서양', '문명', '을', '비교', '하', '며', '두', '문명', '의', '탄생', ',', '만남', ',', '융합', '을', '그린', '책', '.', '과거', '문명', '탐구', '부터', '문명', '의', '발전', '방향', '까지', '살폈', '다', '.', '책', '이름', '에', '맞', '게', '일방통행', '적', '역사', '서술', '이', '아니', '라', '시', '·', '공간', '을', '넘나들', '며', '역사', '현장', '을', '누빈다', '.', '역사', '를', '현실', '과', '함께', '이해', '하', '려는', '저자', '의', '문제', '의식', '이', '돋보인다', '.', '(', '남경태', '지음', ',', '휴머니스트', ',', '744', '쪽', ',', '3', '만', '8000', '원', ')', '◇', '영원', '의', '철학', '=‘', '영원', '의', '철학', '’', '은', '16', '세기', '에', '처음', '언급', '된', '‘', '모든', '위대', '한', '종교', '의', '본질', '적', '이', '고', '공통', '된', '핵심', '진리', '’', '로', ',', '세계', '의', '종교', '적', '전통', '이', '공유', '하', '는', '세계관', ',', '인간관', ',', '윤리관', '을', '말', '한다', '.', '《', '멋진', '신세계', '》', '의', '올더스', '헉슬리', '가', '420', '개', '의', '동서고금', '문헌', '을', '통해', '영원', '의', '철학', '을', '소개', '한다', '.', '인용문', '만', '읽', '어도', '흥미', '로운', '종교', '서', '이자', '명상', '서', '.', '(', '조옥', '경', '옮김', ',', '김영사', ',', '528', '쪽', ',', '1', '만', '9800', '원', ')', '◇', '대한민국', '5', '천', '년', '역사', '리더십', '을', '말', '한다', '=', '고', '조선', '의', '단군', '부터', '이명박', '전', '대통령', '에', '이르', '는', '우리', '민족', '의', '리더십', '을', '다룬', '책', '.', '리더십', '의', '관점', '에서', '우리', '역사', '를', '바라보', '며', '역사', '속', '사건', '들', '에서', '리더십', '이', '어떤', '힘', '을', '발휘', '했', '는지', '살펴보', '고', '홍익인간', ',', '민족주의', ',', '민주주의', '등', '우리', '민족', '고유', '의', '리더십', 'DNA', '여덟', '가지', '를', '제시', '한다', '.', '(', '최익용', '지음', ',', '옥당', ',', '436', '쪽', ',', '2', '만', '2000', '원', ')', '◇', '한국', '자본주의', '모델', '=', '진보', '적', '한국', '경제', '론', '의', '이론', '과', '참여', '를', '주도', '해', '온', '저자', '가', '광복', '이후', '박근혜', '정부', '까지', '70', '여', '년', '의', '대한민국', '경제', '·', '자본주의', '모델', '을', '분석', '한', '책', '.', '이승만', '정부', '의', '대미', '의존', '경제', ',', '박정희', '개발주의', '의', '명암', '을', '살펴보', '면서', '민주', '화', '이후', '의', '한국', '자본주의', '와', '외환', '위기', '후', '경제', '민주', '화', '와', '한국', '경제', '의', '전망', '을', '탐색', '한다', '.', '(', '이병천', '지음', ',', '책', '세상', ',', '480', '쪽', ',', '2', '만', '5000', '원', ')'], 'question': ['영원', '의', '철학', '에', '대한', '소개', '의', '글', '을', '쓰', '기', '위해', '참조', '한', '문헌', '의', '수', '는', '?'], 'answers': [{'start': 238, 'end': 239}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.TokenizedKoMRC.load('data/train.json')\n",
    "train_dataset, eval_dataset = datasets.TokenizedKoMRC.split(dataset)\n",
    "print(\"Number of Questions\", len(train_dataset), len(eval_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['420', '개']\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Vocab: 100%|██████████| 12037/12037 [00:16<00:00, 717.17it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answers'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/8k/shn9vyrx7js7vvlvx41fz4gc0000gn/T/ipykernel_22622/580945414.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mtokenizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuild_vocab\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample2ids\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_dataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/groom8/KoreanMRC/ko_mrc/utils.py\u001B[0m in \u001B[0;36msample2ids\u001B[0;34m(self, sample)\u001B[0m\n\u001B[1;32m     72\u001B[0m         \u001B[0mtoken_type_ids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquestion\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 74\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0msample\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'answers'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     75\u001B[0m             \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msample\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'answers'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     76\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'start'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mquestion\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_length\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'answers'"
     ]
    }
   ],
   "source": [
    "tokenizer = utils.Tokenizer.build_vocab(dataset)\n",
    "print(tokenizer.sample2ids(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = utils.TokenizerWrapperDataset(train_dataset, tokenizer)\n",
    "eval_dataset = utils.TokenizerWrapperDataset(eval_dataset, tokenizer)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = utils.Collator(tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator, num_workers=4)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False, collate_fn=collator, num_workers=4)\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['input_ids'])\n",
    "print(list(batch.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig(\n",
    "     vocab_size=tokenizer.vocab_size,\n",
    "     max_position_embeddings=1024,\n",
    "     hidden_size=512,\n",
    "     num_hidden_layers=6,\n",
    "     num_attention_heads=8,\n",
    "     intermediate_size=2048\n",
    ")\n",
    "model = models.BertForQuestionAnswering(config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='review')\n",
    "os.makedirs('dump', exist_ok=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 31):\n",
    "    print(\"Epoch\", epoch)\n",
    "    for batch in tqdm(train_loader, desc='Train'):\n",
    "        del batch['guid'], batch['context'], batch['question'], batch['position']\n",
    "        batch = {key: value.cuda() for key, value in batch.items()}\n",
    "        start = batch.pop('start')\n",
    "        end = batch.pop('end')\n",
    "        \n",
    "        start_logits, end_logits = model(**batch)\n",
    "        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.)\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "        writer.add_scalar('Train Loss', loss.item(), step)\n",
    "        del batch, start, end, start_logits, end_logits, loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluation\"):\n",
    "            del batch['guid'], batch['context'], batch['question'], batch['position']\n",
    "            batch = {key: value.cuda() for key, value in batch.items()}\n",
    "            start = batch.pop('start')\n",
    "            end = batch.pop('end')\n",
    "\n",
    "            start_logits, end_logits = model(**batch)\n",
    "            loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            del batch, start, end, start_logits, end_logits, loss\n",
    "        loss = sum(losses) / len(losses)\n",
    "        writer.add_scalar('Eval Loss', loss, step)\n",
    "\n",
    "    model.save_pretrained(f'dump/model.{epoch}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.BertForQuestionAnswering.from_pretrained('dump/model.30')\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, raw_sample in zip(range(1, 4), train_dataset):\n",
    "    sample = dict(raw_sample) \n",
    "    context = sample.pop('context')\n",
    "    question = sample.pop('question')\n",
    "    position = sample.pop('position')\n",
    "    start, end = sample.pop('start'), sample.pop('end')\n",
    "    del sample['guid']\n",
    "\n",
    "    sample = {key: value.cuda()[None, :] for key, value in sample.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = model(**sample)\n",
    "    start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
    "\n",
    "    print(f'------{idx}------')\n",
    "    print('Context:', context)\n",
    "    print('Question:', question)\n",
    "    print('Answer:', tokenizer.logits2answer(raw_sample, start_logits, end_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.TokenizedKoMRC.load('data/test.json')\n",
    "test_dataset = utils.TokenizerWrapperDataset(test_dataset, tokenizer)\n",
    "print(\"Number of Questions\", len(test_dataset))\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "os.makedirs('out', exist_ok=True)\n",
    "with torch.no_grad(), open('out/baseline.csv', 'w') as fd:\n",
    "    writer = csv.writer(fd)\n",
    "    writer.writerow(['Id', 'Predicted'])\n",
    "\n",
    "    rows = []\n",
    "    for raw_sample in tqdm(test_dataset, \"Testing\"):\n",
    "        sample = dict(raw_sample) \n",
    "        guid = sample.pop('guid')\n",
    "        context = sample.pop('context')\n",
    "        position = sample.pop('position')\n",
    "        start, end = sample.pop('start'), sample.pop('end')\n",
    "        del sample['question']\n",
    "\n",
    "        sample = {key: value.cuda()[None, :] for key, value in sample.items()}\n",
    "        start_logits, end_logits = model(**sample)\n",
    "        start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
    "\n",
    "        rows.append([guid, tokenizer.logits2answer(raw_sample, start_logits, end_logits)])\n",
    "    \n",
    "    writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}