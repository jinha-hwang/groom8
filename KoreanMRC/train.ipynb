{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get install -y g++ openjdk-8-jdk python3-dev curl git build-essential\n",
    "! pip install konlpy \"tweepy<4.0.0\"\n",
    "! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ko_mrc.datasets as datasets\n",
    "import ko_mrc.utils as utils\n",
    "import ko_mrc.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Questions 12037\n",
      "{'guid': '798db07f0b9046759deed9d4a35ce31e', 'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.', 'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?', 'answers': [{'text': '한 달가량', 'answer_start': 478}, {'text': '한 달', 'answer_start': 478}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.KoMRC.load('data/train.json')\n",
    "print(\"Number of Questions\", len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Questions 10834 1203\n",
      "{'guid': '1ad2029a2cae445498692ba3e9256116', 'context_original': '지난 11월 14일 토요일, 클룩(KLOOK)은 진에어와 함께 관광 비행 및 내년 홍콩 왕복 항공권을 증정하는 ‘미리 즐기는 홍콩원정대’ 항공여행을 진행했다. 클룩은 본 여행을 위해, 진에어 전세기 운항을 주도해 실현시켰다. 특히 이번 상품은 홍콩 여행 테마를 주제로 잃어버린 여행의 설렘을 다시 느낄 수 있도록 한 관광 비행과 얼리버드 홍콩 왕복 항공권을 결합시켜 예약이 조기 매진되는 등 화제를 불러일으킨 패키지다. 오랜만의 비행을 맞이해, 클룩과 진에어 외에도 유관 기관의 협조가 빛을 발했다. 인천공항에서는 미니 콘서트와 국내선 시설 지원을, 제주공항 관제소에서는 한라산 및 제주도 전역 저공비행 허가를 밀어줬다. 여기에 더해, 홍콩관광청에서도 홍콩 여행을 필수적인 가이드북과 컬러링북 등 특전들이 참가자들에게 제공했다. 홍콩원정대 이름에 걸맞게 홍콩 명물 제니쿠키 역시 서비스되었다. 이처럼 기업과 기관이 협조해 진행한 덕분에, 저렴한 비용에 역대급 특전 제공이 가능했다. 홍콩 테마의 관광 비행 1인과 홍콩 왕복 항공권 1매, 클룩의 홍콩 여행 상품권 10만원권을 포함한 가격이 26만 9천원, 관광 비행 1인과 홍콩 왕복 항공권 2매, 클룩 홍콩 여행 상품권 15만원 포함 패키지는 39만 9천원으로 제공되었다. 관광 비행만 원하는 경우를 위해서는 클룩의 국내 여행 상품권 5만원권을 포함한 16만 9천원의 패키지가 선택 가능했다. 함께 제공되는 클룩 여행상품권은 현지 어트랙션이나 입장권은 물론 호텔과 렌터카 등 다양한 여행상품을 구매할 수 있다. 또한 홍콩 왕복 항공권과 클룩 상품권의 경우 2022년 3월까지의 넉넉한 유효기간으로 제공되어, 코로나19로 유효기간 내 사용이 불가능한 경우 유효기간 연장이 가능하다. 단, 홍콩 항공권은 사용 당시 유류할증료와 공항세만 별도 지불하면 된다. 홍콩원정대 전세기는 오후 3시, 인천국제공항을 이륙해 광주, 제주, 부산, 대구 등을 하늘에서 둘러본 뒤 다시 인천공항으로 돌아왔다. 약 2시간의 비행을 통해, 참가자들은 항공기로 오가던 여행의 추억을 되새겼다. 비행 중 진에어에서는 중화덮밥을 기내식으로 제공했으며, 승무원 엔터테인먼트 팀 ‘랄라진스’의 기내 콘서트와 김포-부산 항공권을 건 경품 이벤트도 성황리에 진행되었다. 한편, 모든 관광 비행은 코로나19 방역 지침을 준수하며 진행되었다. 특히 기내 좌석은 실제 탑승 가능 좌석인 189석의 70%인 132석만 운영되었다. 더불어 탑승객 중 가장 어린 승객은 6살, 최고령자는 84살이라는 이색적인 기록도 남은 비행이기도 했다. 클룩 이준호 한국 지사장은 “여행이 다시 시작되는 설레는 순간을 경험할 수 있도록 하늘에서 즐기는 특별한 여행 경험과 홍콩 왕복 항공권, 클룩 상품권까지 풍성한 혜택을 준비했다”며 “잃어버린 여행이 다시 일상 속 즐거움으로 돌아올 수 있는 그날까지 국내외 다양한 체험 상품들을 선보일 것”이라고 말했다. 또한 향후 다른 FSC, LCC 항공사와의 협업과 관광업계 및 유관기관과의 협조를 바탕으로 여행이 그리운 고객들에게 실질적인 혜택이 돌아갈 이벤트의 주선을 클룩이 주선할 것임을 약속했다.', 'context_position': [(0, 2), (3, 5), (5, 6), (7, 9), (9, 10), (11, 14), (14, 15), (16, 17), (17, 18), (18, 19), (19, 24), (24, 25), (25, 26), (27, 30), (30, 31), (32, 34), (35, 37), (38, 40), (41, 42), (43, 45), (46, 48), (49, 51), (52, 54), (54, 55), (55, 56), (57, 59), (59, 60), (60, 61), (62, 63), (63, 65), (66, 68), (68, 69), (70, 72), (72, 75), (75, 76), (77, 79), (79, 81), (81, 82), (83, 85), (85, 86), (86, 87), (87, 88), (89, 90), (90, 91), (91, 92), (93, 94), (95, 97), (97, 98), (99, 101), (101, 102), (103, 106), (107, 110), (111, 113), (113, 114), (115, 117), (117, 118), (119, 121), (121, 123), (123, 124), (124, 125), (126, 128), (129, 131), (132, 134), (134, 135), (136, 138), (139, 141), (142, 144), (144, 145), (146, 148), (148, 149), (150, 154), (155, 157), (157, 158), (159, 161), (161, 162), (163, 165), (166, 168), (169, 170), (171, 172), (172, 174), (175, 176), (177, 179), (180, 182), (182, 183), (184, 186), (186, 188), (189, 191), (192, 194), (195, 197), (197, 198), (198, 199), (200, 202), (202, 204), (205, 207), (207, 208), (209, 211), (212, 214), (214, 215), (215, 216), (217, 218), (219, 221), (221, 222), (223, 228), (229, 232), (232, 233), (233, 234), (235, 238), (238, 239), (240, 242), (242, 243), (244, 247), (247, 248), (249, 250), (250, 251), (251, 252), (253, 256), (257, 258), (258, 259), (259, 260), (261, 263), (264, 266), (266, 267), (268, 270), (270, 271), (272, 273), (273, 274), (275, 277), (277, 278), (278, 279), (280, 282), (282, 284), (284, 286), (286, 287), (288, 290), (291, 294), (294, 295), (296, 299), (300, 302), (303, 305), (305, 306), (306, 307), (308, 310), (310, 312), (313, 316), (316, 318), (318, 319), (320, 323), (324, 325), (326, 329), (330, 332), (333, 337), (338, 340), (340, 341), (342, 345), (345, 346), (346, 347), (348, 350), (350, 351), (352, 353), (353, 354), (354, 355), (356, 358), (358, 360), (360, 361), (361, 363), (363, 364), (365, 367), (368, 370), (370, 371), (372, 374), (374, 375), (375, 376), (377, 381), (381, 382), (383, 385), (385, 387), (388, 389), (390, 392), (392, 393), (393, 394), (395, 398), (398, 399), (399, 401), (402, 404), (404, 405), (405, 406), (406, 407), (408, 410), (410, 413), (414, 416), (416, 417), (418, 420), (420, 421), (422, 424), (425, 427), (428, 430), (430, 432), (433, 435), (436, 439), (439, 440), (440, 441), (441, 442), (442, 443), (444, 445), (445, 447), (448, 450), (450, 451), (452, 454), (454, 455), (456, 458), (458, 459), (460, 462), (462, 463), (464, 466), (466, 467), (467, 468), (469, 471), (471, 472), (473, 475), (475, 476), (477, 479), (479, 480), (481, 483), (484, 486), (486, 487), (488, 490), (490, 491), (491, 492), (492, 493), (494, 496), (497, 499), (499, 500), (501, 503), (504, 506), (507, 508), (508, 510), (511, 513), (514, 516), (517, 520), (521, 522), (522, 523), (523, 524), (525, 526), (526, 527), (527, 528), (529, 531), (532, 534), (535, 538), (539, 541), (541, 542), (542, 543), (543, 544), (544, 545), (546, 548), (548, 549), (550, 552), (552, 553), (554, 556), (556, 557), (558, 559), (559, 560), (560, 561), (561, 562), (563, 565), (566, 568), (569, 570), (570, 572), (573, 575), (576, 578), (579, 582), (583, 584), (584, 585), (585, 586), (587, 588), (588, 589), (590, 592), (593, 595), (596, 599), (600, 602), (602, 603), (603, 604), (605, 607), (608, 611), (611, 612), (613, 615), (615, 616), (617, 618), (618, 619), (619, 620), (620, 622), (623, 625), (625, 626), (626, 627), (627, 628), (628, 629), (630, 632), (633, 635), (635, 636), (637, 639), (639, 640), (641, 643), (643, 644), (645, 648), (648, 649), (650, 651), (651, 652), (652, 653), (654, 656), (657, 659), (660, 663), (664, 665), (665, 666), (666, 667), (667, 668), (668, 669), (670, 672), (672, 673), (674, 676), (676, 677), (678, 679), (679, 680), (680, 681), (681, 682), (683, 686), (686, 687), (688, 690), (691, 693), (693, 694), (694, 695), (695, 696), (697, 699), (700, 702), (702, 703), (703, 704), (705, 706), (706, 707), (708, 710), (710, 713), (713, 714), (715, 717), (718, 722), (722, 724), (725, 728), (728, 729), (730, 732), (733, 735), (735, 736), (737, 740), (741, 742), (743, 745), (745, 746), (747, 749), (749, 751), (751, 752), (753, 755), (755, 756), (757, 758), (759, 760), (760, 761), (761, 762), (763, 765), (766, 768), (769, 771), (772, 774), (774, 775), (775, 776), (777, 778), (778, 779), (780, 783), (783, 784), (785, 787), (788, 792), (792, 793), (794, 795), (795, 796), (796, 798), (798, 799), (800, 802), (802, 803), (804, 806), (806, 808), (808, 810), (811, 813), (813, 814), (814, 815), (815, 816), (817, 820), (820, 822), (822, 823), (824, 826), (826, 828), (829, 830), (831, 833), (833, 834), (835, 836), (836, 838), (838, 839), (840, 842), (843, 845), (845, 847), (848, 850), (850, 851), (852, 854), (854, 855), (855, 856), (856, 857), (858, 859), (859, 860), (861, 863), (864, 866), (866, 867), (867, 868), (869, 871), (872, 874), (875, 877), (877, 880), (880, 881), (882, 885), (885, 886), (887, 889), (890, 892), (892, 893), (893, 894), (895, 897), (897, 898), (899, 901), (901, 904), (905, 908), (908, 909), (910, 912), (913, 914), (914, 915), (915, 916), (917, 919), (919, 923), (923, 924), (925, 927), (927, 928), (929, 931), (931, 932), (933, 935), (935, 936), (937, 939), (939, 940), (941, 943), (944, 945), (945, 946), (947, 949), (949, 951), (952, 955), (956, 957), (958, 960), (961, 963), (963, 965), (965, 967), (968, 971), (971, 972), (972, 973), (974, 975), (976, 977), (977, 979), (979, 980), (981, 983), (983, 984), (985, 987), (987, 988), (989, 992), (992, 993), (993, 994), (995, 998), (998, 999), (1000, 1002), (1002, 1003), (1004, 1006), (1006, 1007), (1008, 1010), (1010, 1011), (1012, 1015), (1015, 1016), (1016, 1017), (1018, 1020), (1021, 1022), (1023, 1026), (1026, 1028), (1028, 1029), (1030, 1032), (1032, 1034), (1034, 1035), (1036, 1039), (1039, 1041), (1042, 1044), (1044, 1047), (1047, 1048), (1049, 1052), (1053, 1055), (1055, 1056), (1056, 1057), (1057, 1058), (1058, 1059), (1060, 1061), (1062, 1063), (1063, 1065), (1065, 1067), (1067, 1068), (1068, 1069), (1070, 1072), (1073, 1076), (1076, 1077), (1078, 1080), (1080, 1081), (1081, 1083), (1084, 1086), (1086, 1087), (1087, 1088), (1089, 1090), (1091, 1093), (1094, 1097), (1097, 1098), (1099, 1102), (1102, 1103), (1104, 1106), (1106, 1107), (1107, 1108), (1108, 1109), (1109, 1110), (1111, 1113), (1113, 1114), (1115, 1117), (1118, 1120), (1121, 1123), (1123, 1124), (1125, 1128), (1128, 1130), (1131, 1133), (1134, 1136), (1136, 1137), (1138, 1139), (1139, 1140), (1140, 1141), (1141, 1142), (1143, 1145), (1145, 1146), (1146, 1147), (1147, 1148), (1148, 1149), (1150, 1152), (1153, 1155), (1156, 1158), (1158, 1159), (1160, 1162), (1163, 1165), (1166, 1168), (1169, 1171), (1171, 1172), (1173, 1176), (1176, 1177), (1177, 1178), (1179, 1181), (1181, 1182), (1182, 1183), (1184, 1187), (1187, 1188), (1188, 1189), (1190, 1192), (1192, 1193), (1193, 1194), (1194, 1195), (1195, 1196), (1197, 1199), (1199, 1200), (1201, 1204), (1205, 1206), (1207, 1209), (1210, 1212), (1213, 1215), (1215, 1216), (1217, 1218), (1218, 1219), (1219, 1220), (1221, 1223), (1223, 1225), (1225, 1226), (1227, 1229), (1229, 1230), (1230, 1231), (1231, 1233), (1234, 1236), (1236, 1237), (1237, 1238), (1239, 1241), (1241, 1242), (1243, 1244), (1244, 1245), (1246, 1248), (1248, 1249), (1249, 1250), (1250, 1251), (1252, 1253), (1253, 1254), (1254, 1255), (1256, 1257), (1257, 1258), (1259, 1262), (1263, 1265), (1266, 1269), (1269, 1270), (1271, 1272), (1272, 1274), (1274, 1275), (1276, 1278), (1279, 1281), (1281, 1282), (1282, 1283), (1284, 1286), (1286, 1287), (1288, 1290), (1290, 1291), (1292, 1294), (1294, 1295), (1296, 1297), (1298, 1299), (1299, 1301), (1302, 1304), (1304, 1306), (1307, 1309), (1309, 1310), (1311, 1313), (1313, 1314), (1315, 1317), (1318, 1320), (1320, 1321), (1322, 1324), (1325, 1327), (1328, 1330), (1330, 1331), (1331, 1332), (1333, 1334), (1334, 1335), (1336, 1339), (1339, 1341), (1342, 1344), (1344, 1345), (1346, 1348), (1348, 1349), (1350, 1352), (1352, 1353), (1353, 1354), (1354, 1355), (1355, 1356), (1357, 1358), (1358, 1362), (1363, 1365), (1365, 1366), (1367, 1369), (1370, 1372), (1373, 1374), (1375, 1378), (1378, 1380), (1381, 1384), (1385, 1386), (1387, 1388), (1388, 1389), (1390, 1392), (1392, 1394), (1395, 1398), (1399, 1401), (1401, 1402), (1403, 1405), (1406, 1408), (1408, 1409), (1409, 1410), (1411, 1414), (1415, 1416), (1416, 1417), (1417, 1418), (1418, 1420), (1421, 1422), (1422, 1423), (1423, 1424), (1424, 1425), (1426, 1428), (1429, 1431), (1432, 1434), (1435, 1438), (1438, 1439), (1440, 1443), (1444, 1447), (1447, 1448), (1448, 1449), (1450, 1452), (1452, 1453), (1454, 1457), (1457, 1458), (1459, 1460), (1461, 1463), (1463, 1465), (1465, 1466), (1466, 1467), (1468, 1470), (1470, 1471), (1472, 1474), (1474, 1476), (1477, 1479), (1479, 1480), (1481, 1484), (1485, 1487), (1487, 1488), (1488, 1490), (1491, 1493), (1493, 1494), (1494, 1495), (1496, 1498), (1498, 1499), (1500, 1503), (1504, 1507), (1507, 1508), (1509, 1511), (1511, 1512), (1513, 1514), (1514, 1515), (1515, 1516), (1517, 1519), (1519, 1520), (1521, 1522), (1522, 1523), (1523, 1524), (1525, 1527), (1527, 1528), (1528, 1529), (1529, 1530)], 'question_original': '패키지 중 가장 싼 것은 얼마인가요?', 'context': ['지난', '11', '월', '14', '일', '토요일', ',', '클', '룩', '(', 'KLOOK', ')', '은', '진에어', '와', '함께', '관광', '비행', '및', '내년', '홍콩', '왕복', '항공', '권', '을', '증정', '하', '는', '‘', '미리', '즐기', '는', '홍콩', '원정대', '’', '항공', '여행', '을', '진행', '했', '다', '.', '클', '룩', '은', '본', '여행', '을', '위해', ',', '진에어', '전세기', '운항', '을', '주도', '해', '실현', '시켰', '다', '.', '특히', '이번', '상품', '은', '홍콩', '여행', '테마', '를', '주제', '로', '잃어버린', '여행', '의', '설렘', '을', '다시', '느낄', '수', '있', '도록', '한', '관광', '비행', '과', '얼리', '버드', '홍콩', '왕복', '항공', '권', '을', '결합', '시켜', '예약', '이', '조기', '매진', '되', '는', '등', '화제', '를', '불러일으킨', '패키지', '다', '.', '오랜만', '의', '비행', '을', '맞이해', ',', '클', '룩', '과', '진에어', '외', '에', '도', '유관', '기관', '의', '협조', '가', '빛', '을', '발했', '다', '.', '인천', '공항', '에서', '는', '미니', '콘서트', '와', '국내선', '시설', '지원', '을', ',', '제주', '공항', '관제소', '에서', '는', '한라산', '및', '제주도', '전역', '저공비행', '허가', '를', '밀어줬', '다', '.', '여기', '에', '더', '해', ',', '홍콩', '관광', '청', '에서', '도', '홍콩', '여행', '을', '필수', '적', '인', '가이드북', '과', '컬러', '링북', '등', '특전', '들', '이', '참가자', '들', '에게', '제공', '했', '다', '.', '홍콩', '원정대', '이름', '에', '걸맞', '게', '홍콩', '명물', '제니', '쿠키', '역시', '서비스', '되', '었', '다', '.', '이', '처럼', '기업', '과', '기관', '이', '협조', '해', '진행', '한', '덕분', '에', ',', '저렴', '한', '비용', '에', '역대', '급', '특전', '제공', '이', '가능', '했', '다', '.', '홍콩', '테마', '의', '관광', '비행', '1', '인과', '홍콩', '왕복', '항공권', '1', '매', ',', '클', '룩', '의', '홍콩', '여행', '상품권', '10', '만', '원', '권', '을', '포함', '한', '가격', '이', '26', '만', '9', '천', '원', ',', '관광', '비행', '1', '인과', '홍콩', '왕복', '항공권', '2', '매', ',', '클', '룩', '홍콩', '여행', '상품권', '15', '만', '원', '포함', '패키지', '는', '39', '만', '9', '천', '원', '으로', '제공', '되', '었', '다', '.', '관광', '비행', '만', '원하', '는', '경우', '를', '위해서', '는', '클', '룩', '의', '국내', '여행', '상품권', '5', '만', '원', '권', '을', '포함', '한', '16', '만', '9', '천', '원', '의', '패키지', '가', '선택', '가능', '했', '다', '.', '함께', '제공', '되', '는', '클', '룩', '여행', '상품권', '은', '현지', '어트랙션', '이나', '입장권', '은', '물론', '호텔', '과', '렌터카', '등', '다양', '한', '여행', '상품', '을', '구매', '할', '수', '있', '다', '.', '또한', '홍콩', '왕복', '항공', '권', '과', '클', '룩', '상품권', '의', '경우', '2022', '년', '3', '월', '까지', '의', '넉넉', '한', '유효', '기간', '으로', '제공', '되', '어', ',', '코로나', '19', '로', '유효', '기간', '내', '사용', '이', '불', '가능', '한', '경우', '유효', '기간', '연장', '이', '가능', '하', '다', '.', '단', ',', '홍콩', '항공', '권', '은', '사용', '당시', '유류', '할증료', '와', '공항세', '만', '별도', '지불', '하', '면', '된다', '.', '홍콩', '원정대', '전세기', '는', '오후', '3', '시', ',', '인천', '국제공항', '을', '이륙', '해', '광주', ',', '제주', ',', '부산', ',', '대구', '등', '을', '하늘', '에서', '둘러본', '뒤', '다시', '인천', '공항', '으로', '돌아왔', '다', '.', '약', '2', '시간', '의', '비행', '을', '통해', ',', '참가자', '들', '은', '항공기', '로', '오가', '던', '여행', '의', '추억', '을', '되새겼', '다', '.', '비행', '중', '진에어', '에서', '는', '중화', '덮밥', '을', '기내식', '으로', '제공', '했으며', ',', '승무원', '엔터', '테', '인', '먼', '트', '팀', '‘', '랄라', '진스', '’', '의', '기내', '콘서트', '와', '김포', '-', '부산', '항공', '권', '을', '건', '경품', '이벤트', '도', '성황리', '에', '진행', '되', '었', '다', '.', '한편', ',', '모든', '관광', '비행', '은', '코로나', '19', '방역', '지침', '을', '준', '수', '하', '며', '진행', '되', '었', '다', '.', '특히', '기내', '좌석', '은', '실제', '탑승', '가능', '좌석', '인', '189', '석', '의', '70', '%', '인', '132', '석', '만', '운영', '되', '었', '다', '.', '더불', '어', '탑승객', '중', '가장', '어린', '승객', '은', '6', '살', ',', '최고', '령자', '는', '84', '살', '이', '라는', '이색', '적', '인', '기록', '도', '남', '은', '비행', '이', '기', '도', '했', '다', '.', '클', '룩', '이준호', '한국', '지사장', '은', '“', '여행', '이', '다시', '시작', '되', '는', '설레', '는', '순간', '을', '경험', '할', '수', '있', '도록', '하늘', '에서', '즐기', '는', '특별', '한', '여행', '경험', '과', '홍콩', '왕복', '항공', '권', ',', '클', '룩', '상품권', '까지', '풍성', '한', '혜택', '을', '준비', '했', '다', '”', '며', '“', '잃어버린', '여행', '이', '다시', '일상', '속', '즐거움', '으로', '돌아올', '수', '있', '는', '그날', '까지', '국내외', '다양', '한', '체험', '상품', '들', '을', '선보일', '것', '”', '이', '라고', '말', '했', '다', '.', '또한', '향후', '다른', 'FSC', ',', 'LCC', '항공사', '와', '의', '협업', '과', '관광업', '계', '및', '유관', '기관', '과', '의', '협조', '를', '바탕', '으로', '여행', '이', '그리운', '고객', '들', '에게', '실질', '적', '인', '혜택', '이', '돌아갈', '이벤트', '의', '주선', '을', '클', '룩', '이', '주선', '할', '것', '임', '을', '약속', '했', '다', '.'], 'question': ['패키지', '중', '가장', '싼', '것', '은', '얼마', '인가요', '?'], 'answers': [{'start': 317, 'end': 321}]}\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.TokenizedKoMRC.load('data/train.json')\n",
    "train_dataset, eval_dataset = datasets.TokenizedKoMRC.split(dataset)\n",
    "print(\"Number of Questions\", len(train_dataset), len(eval_dataset))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16', '만', '9', '천', '원']\n"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[0]\n",
    "print(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Vocab: 100%|██████████| 12037/12037 [00:46<00:00, 260.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guid': '1ad2029a2cae445498692ba3e9256116', 'context': '지난 11월 14일 토요일, 클룩(KLOOK)은 진에어와 함께 관광 비행 및 내년 홍콩 왕복 항공권을 증정하는 ‘미리 즐기는 홍콩원정대’ 항공여행을 진행했다. 클룩은 본 여행을 위해, 진에어 전세기 운항을 주도해 실현시켰다. 특히 이번 상품은 홍콩 여행 테마를 주제로 잃어버린 여행의 설렘을 다시 느낄 수 있도록 한 관광 비행과 얼리버드 홍콩 왕복 항공권을 결합시켜 예약이 조기 매진되는 등 화제를 불러일으킨 패키지다. 오랜만의 비행을 맞이해, 클룩과 진에어 외에도 유관 기관의 협조가 빛을 발했다. 인천공항에서는 미니 콘서트와 국내선 시설 지원을, 제주공항 관제소에서는 한라산 및 제주도 전역 저공비행 허가를 밀어줬다. 여기에 더해, 홍콩관광청에서도 홍콩 여행을 필수적인 가이드북과 컬러링북 등 특전들이 참가자들에게 제공했다. 홍콩원정대 이름에 걸맞게 홍콩 명물 제니쿠키 역시 서비스되었다. 이처럼 기업과 기관이 협조해 진행한 덕분에, 저렴한 비용에 역대급 특전 제공이 가능했다. 홍콩 테마의 관광 비행 1인과 홍콩 왕복 항공권 1매, 클룩의 홍콩 여행 상품권 10만원권을 포함한 가격이 26만 9천원, 관광 비행 1인과 홍콩 왕복 항공권 2매, 클룩 홍콩 여행 상품권 15만원 포함 패키지는 39만 9천원으로 제공되었다. 관광 비행만 원하는 경우를 위해서는 클룩의 국내 여행 상품권 5만원권을 포함한 16만 9천원의 패키지가 선택 가능했다. 함께 제공되는 클룩 여행상품권은 현지 어트랙션이나 입장권은 물론 호텔과 렌터카 등 다양한 여행상품을 구매할 수 있다. 또한 홍콩 왕복 항공권과 클룩 상품권의 경우 2022년 3월까지의 넉넉한 유효기간으로 제공되어, 코로나19로 유효기간 내 사용이 불가능한 경우 유효기간 연장이 가능하다. 단, 홍콩 항공권은 사용 당시 유류할증료와 공항세만 별도 지불하면 된다. 홍콩원정대 전세기는 오후 3시, 인천국제공항을 이륙해 광주, 제주, 부산, 대구 등을 하늘에서 둘러본 뒤 다시 인천공항으로 돌아왔다. 약 2시간의 비행을 통해, 참가자들은 항공기로 오가던 여행의 추억을 되새겼다. 비행 중 진에어에서는 중화덮밥을 기내식으로 제공했으며, 승무원 엔터테인먼트 팀 ‘랄라진스’의 기내 콘서트와 김포-부산 항공권을 건 경품 이벤트도 성황리에 진행되었다. 한편, 모든 관광 비행은 코로나19 방역 지침을 준수하며 진행되었다. 특히 기내 좌석은 실제 탑승 가능 좌석인 189석의 70%인 132석만 운영되었다. 더불어 탑승객 중 가장 어린 승객은 6살, 최고령자는 84살이라는 이색적인 기록도 남은 비행이기도 했다. 클룩 이준호 한국 지사장은 “여행이 다시 시작되는 설레는 순간을 경험할 수 있도록 하늘에서 즐기는 특별한 여행 경험과 홍콩 왕복 항공권, 클룩 상품권까지 풍성한 혜택을 준비했다”며 “잃어버린 여행이 다시 일상 속 즐거움으로 돌아올 수 있는 그날까지 국내외 다양한 체험 상품들을 선보일 것”이라고 말했다. 또한 향후 다른 FSC, LCC 항공사와의 협업과 관광업계 및 유관기관과의 협조를 바탕으로 여행이 그리운 고객들에게 실질적인 혜택이 돌아갈 이벤트의 주선을 클룩이 주선할 것임을 약속했다.', 'question': '패키지 중 가장 싼 것은 얼마인가요?', 'position': [(0, 2), (3, 5), (5, 6), (7, 9), (9, 10), (11, 14), (14, 15), (16, 17), (17, 18), (18, 19), (19, 24), (24, 25), (25, 26), (27, 30), (30, 31), (32, 34), (35, 37), (38, 40), (41, 42), (43, 45), (46, 48), (49, 51), (52, 54), (54, 55), (55, 56), (57, 59), (59, 60), (60, 61), (62, 63), (63, 65), (66, 68), (68, 69), (70, 72), (72, 75), (75, 76), (77, 79), (79, 81), (81, 82), (83, 85), (85, 86), (86, 87), (87, 88), (89, 90), (90, 91), (91, 92), (93, 94), (95, 97), (97, 98), (99, 101), (101, 102), (103, 106), (107, 110), (111, 113), (113, 114), (115, 117), (117, 118), (119, 121), (121, 123), (123, 124), (124, 125), (126, 128), (129, 131), (132, 134), (134, 135), (136, 138), (139, 141), (142, 144), (144, 145), (146, 148), (148, 149), (150, 154), (155, 157), (157, 158), (159, 161), (161, 162), (163, 165), (166, 168), (169, 170), (171, 172), (172, 174), (175, 176), (177, 179), (180, 182), (182, 183), (184, 186), (186, 188), (189, 191), (192, 194), (195, 197), (197, 198), (198, 199), (200, 202), (202, 204), (205, 207), (207, 208), (209, 211), (212, 214), (214, 215), (215, 216), (217, 218), (219, 221), (221, 222), (223, 228), (229, 232), (232, 233), (233, 234), (235, 238), (238, 239), (240, 242), (242, 243), (244, 247), (247, 248), (249, 250), (250, 251), (251, 252), (253, 256), (257, 258), (258, 259), (259, 260), (261, 263), (264, 266), (266, 267), (268, 270), (270, 271), (272, 273), (273, 274), (275, 277), (277, 278), (278, 279), (280, 282), (282, 284), (284, 286), (286, 287), (288, 290), (291, 294), (294, 295), (296, 299), (300, 302), (303, 305), (305, 306), (306, 307), (308, 310), (310, 312), (313, 316), (316, 318), (318, 319), (320, 323), (324, 325), (326, 329), (330, 332), (333, 337), (338, 340), (340, 341), (342, 345), (345, 346), (346, 347), (348, 350), (350, 351), (352, 353), (353, 354), (354, 355), (356, 358), (358, 360), (360, 361), (361, 363), (363, 364), (365, 367), (368, 370), (370, 371), (372, 374), (374, 375), (375, 376), (377, 381), (381, 382), (383, 385), (385, 387), (388, 389), (390, 392), (392, 393), (393, 394), (395, 398), (398, 399), (399, 401), (402, 404), (404, 405), (405, 406), (406, 407), (408, 410), (410, 413), (414, 416), (416, 417), (418, 420), (420, 421), (422, 424), (425, 427), (428, 430), (430, 432), (433, 435), (436, 439), (439, 440), (440, 441), (441, 442), (442, 443), (444, 445), (445, 447), (448, 450), (450, 451), (452, 454), (454, 455), (456, 458), (458, 459), (460, 462), (462, 463), (464, 466), (466, 467), (467, 468), (469, 471), (471, 472), (473, 475), (475, 476), (477, 479), (479, 480), (481, 483), (484, 486), (486, 487), (488, 490), (490, 491), (491, 492), (492, 493), (494, 496), (497, 499), (499, 500), (501, 503), (504, 506), (507, 508), (508, 510), (511, 513), (514, 516), (517, 520), (521, 522), (522, 523), (523, 524), (525, 526), (526, 527), (527, 528), (529, 531), (532, 534), (535, 538), (539, 541), (541, 542), (542, 543), (543, 544), (544, 545), (546, 548), (548, 549), (550, 552), (552, 553), (554, 556), (556, 557), (558, 559), (559, 560), (560, 561), (561, 562), (563, 565), (566, 568), (569, 570), (570, 572), (573, 575), (576, 578), (579, 582), (583, 584), (584, 585), (585, 586), (587, 588), (588, 589), (590, 592), (593, 595), (596, 599), (600, 602), (602, 603), (603, 604), (605, 607), (608, 611), (611, 612), (613, 615), (615, 616), (617, 618), (618, 619), (619, 620), (620, 622), (623, 625), (625, 626), (626, 627), (627, 628), (628, 629), (630, 632), (633, 635), (635, 636), (637, 639), (639, 640), (641, 643), (643, 644), (645, 648), (648, 649), (650, 651), (651, 652), (652, 653), (654, 656), (657, 659), (660, 663), (664, 665), (665, 666), (666, 667), (667, 668), (668, 669), (670, 672), (672, 673), (674, 676), (676, 677), (678, 679), (679, 680), (680, 681), (681, 682), (683, 686), (686, 687), (688, 690), (691, 693), (693, 694), (694, 695), (695, 696), (697, 699), (700, 702), (702, 703), (703, 704), (705, 706), (706, 707), (708, 710), (710, 713), (713, 714), (715, 717), (718, 722), (722, 724), (725, 728), (728, 729), (730, 732), (733, 735), (735, 736), (737, 740), (741, 742), (743, 745), (745, 746), (747, 749), (749, 751), (751, 752), (753, 755), (755, 756), (757, 758), (759, 760), (760, 761), (761, 762), (763, 765), (766, 768), (769, 771), (772, 774), (774, 775), (775, 776), (777, 778), (778, 779), (780, 783), (783, 784), (785, 787), (788, 792), (792, 793), (794, 795), (795, 796), (796, 798), (798, 799), (800, 802), (802, 803), (804, 806), (806, 808), (808, 810), (811, 813), (813, 814), (814, 815), (815, 816), (817, 820), (820, 822), (822, 823), (824, 826), (826, 828), (829, 830), (831, 833), (833, 834), (835, 836), (836, 838), (838, 839), (840, 842), (843, 845), (845, 847), (848, 850), (850, 851), (852, 854), (854, 855), (855, 856), (856, 857), (858, 859), (859, 860), (861, 863), (864, 866), (866, 867), (867, 868), (869, 871), (872, 874), (875, 877), (877, 880), (880, 881), (882, 885), (885, 886), (887, 889), (890, 892), (892, 893), (893, 894), (895, 897), (897, 898), (899, 901), (901, 904), (905, 908), (908, 909), (910, 912), (913, 914), (914, 915), (915, 916), (917, 919), (919, 923), (923, 924), (925, 927), (927, 928), (929, 931), (931, 932), (933, 935), (935, 936), (937, 939), (939, 940), (941, 943), (944, 945), (945, 946), (947, 949), (949, 951), (952, 955), (956, 957), (958, 960), (961, 963), (963, 965), (965, 967), (968, 971), (971, 972), (972, 973), (974, 975), (976, 977), (977, 979), (979, 980), (981, 983), (983, 984), (985, 987), (987, 988), (989, 992), (992, 993), (993, 994), (995, 998), (998, 999), (1000, 1002), (1002, 1003), (1004, 1006), (1006, 1007), (1008, 1010), (1010, 1011), (1012, 1015), (1015, 1016), (1016, 1017), (1018, 1020), (1021, 1022), (1023, 1026), (1026, 1028), (1028, 1029), (1030, 1032), (1032, 1034), (1034, 1035), (1036, 1039), (1039, 1041), (1042, 1044), (1044, 1047), (1047, 1048), (1049, 1052), (1053, 1055), (1055, 1056), (1056, 1057), (1057, 1058), (1058, 1059), (1060, 1061), (1062, 1063), (1063, 1065), (1065, 1067), (1067, 1068), (1068, 1069), (1070, 1072), (1073, 1076), (1076, 1077), (1078, 1080), (1080, 1081), (1081, 1083), (1084, 1086), (1086, 1087), (1087, 1088), (1089, 1090), (1091, 1093), (1094, 1097), (1097, 1098), (1099, 1102), (1102, 1103), (1104, 1106), (1106, 1107), (1107, 1108), (1108, 1109), (1109, 1110), (1111, 1113), (1113, 1114), (1115, 1117), (1118, 1120), (1121, 1123), (1123, 1124), (1125, 1128), (1128, 1130), (1131, 1133), (1134, 1136), (1136, 1137), (1138, 1139), (1139, 1140), (1140, 1141), (1141, 1142), (1143, 1145), (1145, 1146), (1146, 1147), (1147, 1148), (1148, 1149), (1150, 1152), (1153, 1155), (1156, 1158), (1158, 1159), (1160, 1162), (1163, 1165), (1166, 1168), (1169, 1171), (1171, 1172), (1173, 1176), (1176, 1177), (1177, 1178), (1179, 1181), (1181, 1182), (1182, 1183), (1184, 1187), (1187, 1188), (1188, 1189), (1190, 1192), (1192, 1193), (1193, 1194), (1194, 1195), (1195, 1196), (1197, 1199), (1199, 1200), (1201, 1204), (1205, 1206), (1207, 1209), (1210, 1212), (1213, 1215), (1215, 1216), (1217, 1218), (1218, 1219), (1219, 1220), (1221, 1223), (1223, 1225), (1225, 1226), (1227, 1229), (1229, 1230), (1230, 1231), (1231, 1233), (1234, 1236), (1236, 1237), (1237, 1238), (1239, 1241), (1241, 1242), (1243, 1244), (1244, 1245), (1246, 1248), (1248, 1249), (1249, 1250), (1250, 1251), (1252, 1253), (1253, 1254), (1254, 1255), (1256, 1257), (1257, 1258), (1259, 1262), (1263, 1265), (1266, 1269), (1269, 1270), (1271, 1272), (1272, 1274), (1274, 1275), (1276, 1278), (1279, 1281), (1281, 1282), (1282, 1283), (1284, 1286), (1286, 1287), (1288, 1290), (1290, 1291), (1292, 1294), (1294, 1295), (1296, 1297), (1298, 1299), (1299, 1301), (1302, 1304), (1304, 1306), (1307, 1309), (1309, 1310), (1311, 1313), (1313, 1314), (1315, 1317), (1318, 1320), (1320, 1321), (1322, 1324), (1325, 1327), (1328, 1330), (1330, 1331), (1331, 1332), (1333, 1334), (1334, 1335), (1336, 1339), (1339, 1341), (1342, 1344), (1344, 1345), (1346, 1348), (1348, 1349), (1350, 1352), (1352, 1353), (1353, 1354), (1354, 1355), (1355, 1356), (1357, 1358), (1358, 1362), (1363, 1365), (1365, 1366), (1367, 1369), (1370, 1372), (1373, 1374), (1375, 1378), (1378, 1380), (1381, 1384), (1385, 1386), (1387, 1388), (1388, 1389), (1390, 1392), (1392, 1394), (1395, 1398), (1399, 1401), (1401, 1402), (1403, 1405), (1406, 1408), (1408, 1409), (1409, 1410), (1411, 1414), (1415, 1416), (1416, 1417), (1417, 1418), (1418, 1420), (1421, 1422), (1422, 1423), (1423, 1424), (1424, 1425), (1426, 1428), (1429, 1431), (1432, 1434), (1435, 1438), (1438, 1439), (1440, 1443), (1444, 1447), (1447, 1448), (1448, 1449), (1450, 1452), (1452, 1453), (1454, 1457), (1457, 1458), (1459, 1460), (1461, 1463), (1463, 1465), (1465, 1466), (1466, 1467), (1468, 1470), (1470, 1471), (1472, 1474), (1474, 1476), (1477, 1479), (1479, 1480), (1481, 1484), (1485, 1487), (1487, 1488), (1488, 1490), (1491, 1493), (1493, 1494), (1494, 1495), (1496, 1498), (1498, 1499), (1500, 1503), (1504, 1507), (1507, 1508), (1509, 1511), (1511, 1512), (1513, 1514), (1514, 1515), (1515, 1516), (1517, 1519), (1519, 1520), (1521, 1522), (1522, 1523), (1523, 1524), (1525, 1527), (1527, 1528), (1528, 1529), (1529, 1530)], 'input_ids': [2, 12371, 449, 1049, 7102, 99, 20, 5578, 1904, 170, 3, 726, 530, 130, 631, 9, 1380, 66, 6488, 13809, 173, 1, 175, 20, 31299, 172, 937, 5503, 18128, 46, 2424, 2878, 21811, 813, 849, 97, 11791, 57, 39, 183, 6602, 2198, 39, 2878, 22920, 191, 813, 885, 97, 219, 242, 14, 15, 6488, 13809, 20, 4360, 885, 97, 372, 66, 31299, 1, 15070, 97, 1337, 93, 1631, 1825, 14, 15, 791, 284, 572, 20, 2878, 885, 12560, 82, 1018, 142, 13155, 885, 41, 33626, 97, 91, 4944, 402, 38, 453, 71, 5503, 18128, 74, 7124, 28082, 2878, 21811, 813, 849, 97, 5542, 3984, 905, 30, 309, 4122, 80, 39, 17, 3569, 82, 27801, 12371, 14, 15, 22306, 41, 18128, 97, 24472, 66, 6488, 13809, 74, 31299, 495, 32, 86, 13851, 256, 41, 2609, 7, 2800, 97, 28865, 14, 15, 671, 908, 11, 39, 13960, 18118, 172, 25917, 3105, 190, 97, 66, 61, 908, 1, 11, 39, 13898, 46, 10, 12739, 1, 1997, 82, 1, 14, 15, 2830, 32, 840, 93, 66, 2878, 5503, 7544, 11, 86, 2878, 885, 97, 783, 145, 198, 35911, 74, 5145, 34932, 17, 13637, 264, 30, 4778, 264, 806, 874, 242, 14, 15, 2878, 22920, 562, 32, 14558, 116, 2878, 32694, 32248, 11385, 2213, 187, 80, 133, 14, 15, 30, 616, 206, 74, 256, 30, 2609, 93, 219, 71, 581, 32, 66, 3071, 71, 322, 32, 4227, 1348, 13637, 874, 30, 330, 242, 14, 15, 2878, 12560, 41, 5503, 18128, 285, 16746, 2878, 21811, 959, 285, 1401, 66, 6488, 13809, 41, 2878, 885, 17256, 624, 384, 226, 849, 97, 2216, 71, 2862, 30, 681, 384, 1513, 1548, 226, 66, 5503, 18128, 285, 16746, 2878, 21811, 959, 63, 1401, 66, 6488, 13809, 2878, 885, 17256, 460, 384, 226, 2216, 12371, 39, 12367, 384, 1513, 1548, 226, 43, 874, 80, 133, 14, 15, 5503, 18128, 384, 3182, 39, 2724, 82, 2635, 39, 6488, 13809, 41, 168, 885, 17256, 246, 384, 226, 849, 97, 2216, 71, 457, 384, 1513, 1548, 226, 41, 12371, 7, 781, 330, 242, 14, 15, 937, 874, 80, 39, 6488, 13809, 885, 17256, 20, 3809, 1, 794, 15483, 20, 909, 4612, 74, 15082, 17, 837, 71, 885, 572, 97, 5152, 407, 402, 38, 14, 15, 887, 2878, 21811, 813, 849, 74, 6488, 13809, 17256, 41, 2724, 11825, 126, 65, 130, 96, 41, 869, 71, 3759, 135, 43, 874, 80, 108, 66, 426, 427, 142, 3759, 135, 302, 2079, 30, 1689, 330, 71, 2724, 3759, 135, 2704, 30, 330, 57, 14, 15, 2841, 66, 2878, 813, 849, 20, 2079, 1293, 26011, 21031, 172, 1, 384, 2073, 3735, 57, 34, 229, 15, 2878, 22920, 1, 39, 1010, 65, 355, 66, 671, 19960, 97, 1305, 93, 2277, 66, 61, 66, 176, 66, 3292, 17, 97, 8144, 11, 23862, 1815, 91, 671, 908, 43, 3242, 14, 15, 223, 63, 1244, 41, 18128, 97, 212, 66, 4778, 264, 20, 8481, 142, 9258, 553, 885, 41, 12983, 97, 1, 14, 15, 18128, 449, 31299, 11, 39, 1918, 34651, 97, 848, 43, 874, 2112, 66, 13696, 853, 854, 198, 36, 855, 513, 183, 35135, 1, 191, 41, 852, 18118, 172, 6273, 1054, 176, 813, 849, 97, 1880, 21237, 2181, 86, 21815, 32, 219, 80, 133, 14, 15, 966, 66, 1206, 5503, 18128, 20, 426, 427, 1151, 6753, 97, 1399, 402, 57, 123, 219, 80, 133, 14, 15, 791, 852, 859, 20, 615, 5703, 330, 859, 198, 28464, 847, 41, 4119, 683, 198, 13244, 847, 384, 1390, 80, 133, 14, 15, 951, 108, 27971, 449, 1049, 5179, 877, 20, 129, 2289, 66, 822, 28811, 39, 4306, 2289, 30, 115, 17242, 145, 198, 462, 86, 1144, 20, 18128, 30, 475, 86, 242, 14, 15, 6488, 13809, 22465, 149, 19385, 20, 425, 885, 30, 91, 12, 80, 39, 23125, 39, 4419, 97, 875, 407, 402, 38, 453, 8144, 11, 2198, 39, 916, 71, 885, 875, 74, 2878, 21811, 813, 849, 66, 6488, 13809, 17256, 96, 645, 71, 926, 97, 1074, 242, 14, 443, 123, 425, 13155, 885, 30, 91, 4915, 1179, 11313, 43, 19754, 402, 38, 39, 5574, 96, 6064, 837, 71, 2322, 572, 264, 97, 5858, 99, 443, 30, 735, 26, 242, 14, 15, 887, 1193, 445, 1, 66, 20617, 811, 172, 41, 1864, 74, 9443, 1038, 46, 13851, 256, 74, 41, 2609, 82, 1802, 43, 885, 30, 1, 827, 264, 806, 2974, 145, 198, 926, 30, 19281, 2181, 41, 6181, 97, 6488, 13809, 30, 6181, 407, 99, 1957, 97, 1603, 242, 14, 15, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'start': 328, 'end': 332}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = utils.Tokenizer.build_vocab(dataset)\n",
    "print(tokenizer.sample2ids(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guid': '1ad2029a2cae445498692ba3e9256116', 'context': '지난 11월 14일 토요일, 클룩(KLOOK)은 진에어와 함께 관광 비행 및 내년 홍콩 왕복 항공권을 증정하는 ‘미리 즐기는 홍콩원정대’ 항공여행을 진행했다. 클룩은 본 여행을 위해, 진에어 전세기 운항을 주도해 실현시켰다. 특히 이번 상품은 홍콩 여행 테마를 주제로 잃어버린 여행의 설렘을 다시 느낄 수 있도록 한 관광 비행과 얼리버드 홍콩 왕복 항공권을 결합시켜 예약이 조기 매진되는 등 화제를 불러일으킨 패키지다. 오랜만의 비행을 맞이해, 클룩과 진에어 외에도 유관 기관의 협조가 빛을 발했다. 인천공항에서는 미니 콘서트와 국내선 시설 지원을, 제주공항 관제소에서는 한라산 및 제주도 전역 저공비행 허가를 밀어줬다. 여기에 더해, 홍콩관광청에서도 홍콩 여행을 필수적인 가이드북과 컬러링북 등 특전들이 참가자들에게 제공했다. 홍콩원정대 이름에 걸맞게 홍콩 명물 제니쿠키 역시 서비스되었다. 이처럼 기업과 기관이 협조해 진행한 덕분에, 저렴한 비용에 역대급 특전 제공이 가능했다. 홍콩 테마의 관광 비행 1인과 홍콩 왕복 항공권 1매, 클룩의 홍콩 여행 상품권 10만원권을 포함한 가격이 26만 9천원, 관광 비행 1인과 홍콩 왕복 항공권 2매, 클룩 홍콩 여행 상품권 15만원 포함 패키지는 39만 9천원으로 제공되었다. 관광 비행만 원하는 경우를 위해서는 클룩의 국내 여행 상품권 5만원권을 포함한 16만 9천원의 패키지가 선택 가능했다. 함께 제공되는 클룩 여행상품권은 현지 어트랙션이나 입장권은 물론 호텔과 렌터카 등 다양한 여행상품을 구매할 수 있다. 또한 홍콩 왕복 항공권과 클룩 상품권의 경우 2022년 3월까지의 넉넉한 유효기간으로 제공되어, 코로나19로 유효기간 내 사용이 불가능한 경우 유효기간 연장이 가능하다. 단, 홍콩 항공권은 사용 당시 유류할증료와 공항세만 별도 지불하면 된다. 홍콩원정대 전세기는 오후 3시, 인천국제공항을 이륙해 광주, 제주, 부산, 대구 등을 하늘에서 둘러본 뒤 다시 인천공항으로 돌아왔다. 약 2시간의 비행을 통해, 참가자들은 항공기로 오가던 여행의 추억을 되새겼다. 비행 중 진에어에서는 중화덮밥을 기내식으로 제공했으며, 승무원 엔터테인먼트 팀 ‘랄라진스’의 기내 콘서트와 김포-부산 항공권을 건 경품 이벤트도 성황리에 진행되었다. 한편, 모든 관광 비행은 코로나19 방역 지침을 준수하며 진행되었다. 특히 기내 좌석은 실제 탑승 가능 좌석인 189석의 70%인 132석만 운영되었다. 더불어 탑승객 중 가장 어린 승객은 6살, 최고령자는 84살이라는 이색적인 기록도 남은 비행이기도 했다. 클룩 이준호 한국 지사장은 “여행이 다시 시작되는 설레는 순간을 경험할 수 있도록 하늘에서 즐기는 특별한 여행 경험과 홍콩 왕복 항공권, 클룩 상품권까지 풍성한 혜택을 준비했다”며 “잃어버린 여행이 다시 일상 속 즐거움으로 돌아올 수 있는 그날까지 국내외 다양한 체험 상품들을 선보일 것”이라고 말했다. 또한 향후 다른 FSC, LCC 항공사와의 협업과 관광업계 및 유관기관과의 협조를 바탕으로 여행이 그리운 고객들에게 실질적인 혜택이 돌아갈 이벤트의 주선을 클룩이 주선할 것임을 약속했다.', 'question': '패키지 중 가장 싼 것은 얼마인가요?', 'position': [(0, 2), (3, 5), (5, 6), (7, 9), (9, 10), (11, 14), (14, 15), (16, 17), (17, 18), (18, 19), (19, 24), (24, 25), (25, 26), (27, 30), (30, 31), (32, 34), (35, 37), (38, 40), (41, 42), (43, 45), (46, 48), (49, 51), (52, 54), (54, 55), (55, 56), (57, 59), (59, 60), (60, 61), (62, 63), (63, 65), (66, 68), (68, 69), (70, 72), (72, 75), (75, 76), (77, 79), (79, 81), (81, 82), (83, 85), (85, 86), (86, 87), (87, 88), (89, 90), (90, 91), (91, 92), (93, 94), (95, 97), (97, 98), (99, 101), (101, 102), (103, 106), (107, 110), (111, 113), (113, 114), (115, 117), (117, 118), (119, 121), (121, 123), (123, 124), (124, 125), (126, 128), (129, 131), (132, 134), (134, 135), (136, 138), (139, 141), (142, 144), (144, 145), (146, 148), (148, 149), (150, 154), (155, 157), (157, 158), (159, 161), (161, 162), (163, 165), (166, 168), (169, 170), (171, 172), (172, 174), (175, 176), (177, 179), (180, 182), (182, 183), (184, 186), (186, 188), (189, 191), (192, 194), (195, 197), (197, 198), (198, 199), (200, 202), (202, 204), (205, 207), (207, 208), (209, 211), (212, 214), (214, 215), (215, 216), (217, 218), (219, 221), (221, 222), (223, 228), (229, 232), (232, 233), (233, 234), (235, 238), (238, 239), (240, 242), (242, 243), (244, 247), (247, 248), (249, 250), (250, 251), (251, 252), (253, 256), (257, 258), (258, 259), (259, 260), (261, 263), (264, 266), (266, 267), (268, 270), (270, 271), (272, 273), (273, 274), (275, 277), (277, 278), (278, 279), (280, 282), (282, 284), (284, 286), (286, 287), (288, 290), (291, 294), (294, 295), (296, 299), (300, 302), (303, 305), (305, 306), (306, 307), (308, 310), (310, 312), (313, 316), (316, 318), (318, 319), (320, 323), (324, 325), (326, 329), (330, 332), (333, 337), (338, 340), (340, 341), (342, 345), (345, 346), (346, 347), (348, 350), (350, 351), (352, 353), (353, 354), (354, 355), (356, 358), (358, 360), (360, 361), (361, 363), (363, 364), (365, 367), (368, 370), (370, 371), (372, 374), (374, 375), (375, 376), (377, 381), (381, 382), (383, 385), (385, 387), (388, 389), (390, 392), (392, 393), (393, 394), (395, 398), (398, 399), (399, 401), (402, 404), (404, 405), (405, 406), (406, 407), (408, 410), (410, 413), (414, 416), (416, 417), (418, 420), (420, 421), (422, 424), (425, 427), (428, 430), (430, 432), (433, 435), (436, 439), (439, 440), (440, 441), (441, 442), (442, 443), (444, 445), (445, 447), (448, 450), (450, 451), (452, 454), (454, 455), (456, 458), (458, 459), (460, 462), (462, 463), (464, 466), (466, 467), (467, 468), (469, 471), (471, 472), (473, 475), (475, 476), (477, 479), (479, 480), (481, 483), (484, 486), (486, 487), (488, 490), (490, 491), (491, 492), (492, 493), (494, 496), (497, 499), (499, 500), (501, 503), (504, 506), (507, 508), (508, 510), (511, 513), (514, 516), (517, 520), (521, 522), (522, 523), (523, 524), (525, 526), (526, 527), (527, 528), (529, 531), (532, 534), (535, 538), (539, 541), (541, 542), (542, 543), (543, 544), (544, 545), (546, 548), (548, 549), (550, 552), (552, 553), (554, 556), (556, 557), (558, 559), (559, 560), (560, 561), (561, 562), (563, 565), (566, 568), (569, 570), (570, 572), (573, 575), (576, 578), (579, 582), (583, 584), (584, 585), (585, 586), (587, 588), (588, 589), (590, 592), (593, 595), (596, 599), (600, 602), (602, 603), (603, 604), (605, 607), (608, 611), (611, 612), (613, 615), (615, 616), (617, 618), (618, 619), (619, 620), (620, 622), (623, 625), (625, 626), (626, 627), (627, 628), (628, 629), (630, 632), (633, 635), (635, 636), (637, 639), (639, 640), (641, 643), (643, 644), (645, 648), (648, 649), (650, 651), (651, 652), (652, 653), (654, 656), (657, 659), (660, 663), (664, 665), (665, 666), (666, 667), (667, 668), (668, 669), (670, 672), (672, 673), (674, 676), (676, 677), (678, 679), (679, 680), (680, 681), (681, 682), (683, 686), (686, 687), (688, 690), (691, 693), (693, 694), (694, 695), (695, 696), (697, 699), (700, 702), (702, 703), (703, 704), (705, 706), (706, 707), (708, 710), (710, 713), (713, 714), (715, 717), (718, 722), (722, 724), (725, 728), (728, 729), (730, 732), (733, 735), (735, 736), (737, 740), (741, 742), (743, 745), (745, 746), (747, 749), (749, 751), (751, 752), (753, 755), (755, 756), (757, 758), (759, 760), (760, 761), (761, 762), (763, 765), (766, 768), (769, 771), (772, 774), (774, 775), (775, 776), (777, 778), (778, 779), (780, 783), (783, 784), (785, 787), (788, 792), (792, 793), (794, 795), (795, 796), (796, 798), (798, 799), (800, 802), (802, 803), (804, 806), (806, 808), (808, 810), (811, 813), (813, 814), (814, 815), (815, 816), (817, 820), (820, 822), (822, 823), (824, 826), (826, 828), (829, 830), (831, 833), (833, 834), (835, 836), (836, 838), (838, 839), (840, 842), (843, 845), (845, 847), (848, 850), (850, 851), (852, 854), (854, 855), (855, 856), (856, 857), (858, 859), (859, 860), (861, 863), (864, 866), (866, 867), (867, 868), (869, 871), (872, 874), (875, 877), (877, 880), (880, 881), (882, 885), (885, 886), (887, 889), (890, 892), (892, 893), (893, 894), (895, 897), (897, 898), (899, 901), (901, 904), (905, 908), (908, 909), (910, 912), (913, 914), (914, 915), (915, 916), (917, 919), (919, 923), (923, 924), (925, 927), (927, 928), (929, 931), (931, 932), (933, 935), (935, 936), (937, 939), (939, 940), (941, 943), (944, 945), (945, 946), (947, 949), (949, 951), (952, 955), (956, 957), (958, 960), (961, 963), (963, 965), (965, 967), (968, 971), (971, 972), (972, 973), (974, 975), (976, 977), (977, 979), (979, 980), (981, 983), (983, 984), (985, 987), (987, 988), (989, 992), (992, 993), (993, 994), (995, 998), (998, 999), (1000, 1002), (1002, 1003), (1004, 1006), (1006, 1007), (1008, 1010), (1010, 1011), (1012, 1015), (1015, 1016), (1016, 1017), (1018, 1020), (1021, 1022), (1023, 1026), (1026, 1028), (1028, 1029), (1030, 1032), (1032, 1034), (1034, 1035), (1036, 1039), (1039, 1041), (1042, 1044), (1044, 1047), (1047, 1048), (1049, 1052), (1053, 1055), (1055, 1056), (1056, 1057), (1057, 1058), (1058, 1059), (1060, 1061), (1062, 1063), (1063, 1065), (1065, 1067), (1067, 1068), (1068, 1069), (1070, 1072), (1073, 1076), (1076, 1077), (1078, 1080), (1080, 1081), (1081, 1083), (1084, 1086), (1086, 1087), (1087, 1088), (1089, 1090), (1091, 1093), (1094, 1097), (1097, 1098), (1099, 1102), (1102, 1103), (1104, 1106), (1106, 1107), (1107, 1108), (1108, 1109), (1109, 1110), (1111, 1113), (1113, 1114), (1115, 1117), (1118, 1120), (1121, 1123), (1123, 1124), (1125, 1128), (1128, 1130), (1131, 1133), (1134, 1136), (1136, 1137), (1138, 1139), (1139, 1140), (1140, 1141), (1141, 1142), (1143, 1145), (1145, 1146), (1146, 1147), (1147, 1148), (1148, 1149), (1150, 1152), (1153, 1155), (1156, 1158), (1158, 1159), (1160, 1162), (1163, 1165), (1166, 1168), (1169, 1171), (1171, 1172), (1173, 1176), (1176, 1177), (1177, 1178), (1179, 1181), (1181, 1182), (1182, 1183), (1184, 1187), (1187, 1188), (1188, 1189), (1190, 1192), (1192, 1193), (1193, 1194), (1194, 1195), (1195, 1196), (1197, 1199), (1199, 1200), (1201, 1204), (1205, 1206), (1207, 1209), (1210, 1212), (1213, 1215), (1215, 1216), (1217, 1218), (1218, 1219), (1219, 1220), (1221, 1223), (1223, 1225), (1225, 1226), (1227, 1229), (1229, 1230), (1230, 1231), (1231, 1233), (1234, 1236), (1236, 1237), (1237, 1238), (1239, 1241), (1241, 1242), (1243, 1244), (1244, 1245), (1246, 1248), (1248, 1249), (1249, 1250), (1250, 1251), (1252, 1253), (1253, 1254), (1254, 1255), (1256, 1257), (1257, 1258), (1259, 1262), (1263, 1265), (1266, 1269), (1269, 1270), (1271, 1272), (1272, 1274), (1274, 1275), (1276, 1278), (1279, 1281), (1281, 1282), (1282, 1283), (1284, 1286), (1286, 1287), (1288, 1290), (1290, 1291), (1292, 1294), (1294, 1295), (1296, 1297), (1298, 1299), (1299, 1301), (1302, 1304), (1304, 1306), (1307, 1309), (1309, 1310), (1311, 1313), (1313, 1314), (1315, 1317), (1318, 1320), (1320, 1321), (1322, 1324), (1325, 1327), (1328, 1330), (1330, 1331), (1331, 1332), (1333, 1334), (1334, 1335), (1336, 1339), (1339, 1341), (1342, 1344), (1344, 1345), (1346, 1348), (1348, 1349), (1350, 1352), (1352, 1353), (1353, 1354), (1354, 1355), (1355, 1356), (1357, 1358), (1358, 1362), (1363, 1365), (1365, 1366), (1367, 1369), (1370, 1372), (1373, 1374), (1375, 1378), (1378, 1380), (1381, 1384), (1385, 1386), (1387, 1388), (1388, 1389), (1390, 1392), (1392, 1394), (1395, 1398), (1399, 1401), (1401, 1402), (1403, 1405), (1406, 1408), (1408, 1409), (1409, 1410), (1411, 1414), (1415, 1416), (1416, 1417), (1417, 1418), (1418, 1420), (1421, 1422), (1422, 1423), (1423, 1424), (1424, 1425), (1426, 1428), (1429, 1431), (1432, 1434), (1435, 1438), (1438, 1439), (1440, 1443), (1444, 1447), (1447, 1448), (1448, 1449), (1450, 1452), (1452, 1453), (1454, 1457), (1457, 1458), (1459, 1460), (1461, 1463), (1463, 1465), (1465, 1466), (1466, 1467), (1468, 1470), (1470, 1471), (1472, 1474), (1474, 1476), (1477, 1479), (1479, 1480), (1481, 1484), (1485, 1487), (1487, 1488), (1488, 1490), (1491, 1493), (1493, 1494), (1494, 1495), (1496, 1498), (1498, 1499), (1500, 1503), (1504, 1507), (1507, 1508), (1509, 1511), (1511, 1512), (1513, 1514), (1514, 1515), (1515, 1516), (1517, 1519), (1519, 1520), (1521, 1522), (1522, 1523), (1523, 1524), (1525, 1527), (1527, 1528), (1528, 1529), (1529, 1530)], 'input_ids': tensor([    2, 12371,   449,  1049,  7102,    99,    20,  5578,  1904,   170,\n",
      "            3,   726,   530,   130,   631,     9,  1380,    66,  6488, 13809,\n",
      "          173,     1,   175,    20, 31299,   172,   937,  5503, 18128,    46,\n",
      "         2424,  2878, 21811,   813,   849,    97, 11791,    57,    39,   183,\n",
      "         6602,  2198,    39,  2878, 22920,   191,   813,   885,    97,   219,\n",
      "          242,    14,    15,  6488, 13809,    20,  4360,   885,    97,   372,\n",
      "           66, 31299,     1, 15070,    97,  1337,    93,  1631,  1825,    14,\n",
      "           15,   791,   284,   572,    20,  2878,   885, 12560,    82,  1018,\n",
      "          142, 13155,   885,    41, 33626,    97,    91,  4944,   402,    38,\n",
      "          453,    71,  5503, 18128,    74,  7124, 28082,  2878, 21811,   813,\n",
      "          849,    97,  5542,  3984,   905,    30,   309,  4122,    80,    39,\n",
      "           17,  3569,    82, 27801, 12371,    14,    15, 22306,    41, 18128,\n",
      "           97, 24472,    66,  6488, 13809,    74, 31299,   495,    32,    86,\n",
      "        13851,   256,    41,  2609,     7,  2800,    97, 28865,    14,    15,\n",
      "          671,   908,    11,    39, 13960, 18118,   172, 25917,  3105,   190,\n",
      "           97,    66,    61,   908,     1,    11,    39, 13898,    46,    10,\n",
      "        12739,     1,  1997,    82,     1,    14,    15,  2830,    32,   840,\n",
      "           93,    66,  2878,  5503,  7544,    11,    86,  2878,   885,    97,\n",
      "          783,   145,   198, 35911,    74,  5145, 34932,    17, 13637,   264,\n",
      "           30,  4778,   264,   806,   874,   242,    14,    15,  2878, 22920,\n",
      "          562,    32, 14558,   116,  2878, 32694, 32248, 11385,  2213,   187,\n",
      "           80,   133,    14,    15,    30,   616,   206,    74,   256,    30,\n",
      "         2609,    93,   219,    71,   581,    32,    66,  3071,    71,   322,\n",
      "           32,  4227,  1348, 13637,   874,    30,   330,   242,    14,    15,\n",
      "         2878, 12560,    41,  5503, 18128,   285, 16746,  2878, 21811,   959,\n",
      "          285,  1401,    66,  6488, 13809,    41,  2878,   885, 17256,   624,\n",
      "          384,   226,   849,    97,  2216,    71,  2862,    30,   681,   384,\n",
      "         1513,  1548,   226,    66,  5503, 18128,   285, 16746,  2878, 21811,\n",
      "          959,    63,  1401,    66,  6488, 13809,  2878,   885, 17256,   460,\n",
      "          384,   226,  2216, 12371,    39, 12367,   384,  1513,  1548,   226,\n",
      "           43,   874,    80,   133,    14,    15,  5503, 18128,   384,  3182,\n",
      "           39,  2724,    82,  2635,    39,  6488, 13809,    41,   168,   885,\n",
      "        17256,   246,   384,   226,   849,    97,  2216,    71,   457,   384,\n",
      "         1513,  1548,   226,    41, 12371,     7,   781,   330,   242,    14,\n",
      "           15,   937,   874,    80,    39,  6488, 13809,   885, 17256,    20,\n",
      "         3809,     1,   794, 15483,    20,   909,  4612,    74, 15082,    17,\n",
      "          837,    71,   885,   572,    97,  5152,   407,   402,    38,    14,\n",
      "           15,   887,  2878, 21811,   813,   849,    74,  6488, 13809, 17256,\n",
      "           41,  2724, 11825,   126,    65,   130,    96,    41,   869,    71,\n",
      "         3759,   135,    43,   874,    80,   108,    66,   426,   427,   142,\n",
      "         3759,   135,   302,  2079,    30,  1689,   330,    71,  2724,  3759,\n",
      "          135,  2704,    30,   330,    57,    14,    15,  2841,    66,  2878,\n",
      "          813,   849,    20,  2079,  1293, 26011, 21031,   172,     1,   384,\n",
      "         2073,  3735,    57,    34,   229,    15,  2878, 22920,     1,    39,\n",
      "         1010,    65,   355,    66,   671, 19960,    97,  1305,    93,  2277,\n",
      "           66,    61,    66,   176,    66,  3292,    17,    97,  8144,    11,\n",
      "        23862,  1815,    91,   671,   908,    43,  3242,    14,    15,   223,\n",
      "           63,  1244,    41, 18128,    97,   212,    66,  4778,   264,    20,\n",
      "         8481,   142,  9258,   553,   885,    41, 12983,    97,     1,    14,\n",
      "           15, 18128,   449, 31299,    11,    39,  1918, 34651,    97,   848,\n",
      "           43,   874,  2112,    66, 13696,   853,   854,   198,    36,   855,\n",
      "          513,   183, 35135,     1,   191,    41,   852, 18118,   172,  6273,\n",
      "         1054,   176,   813,   849,    97,  1880, 21237,  2181,    86, 21815,\n",
      "           32,   219,    80,   133,    14,    15,   966,    66,  1206,  5503,\n",
      "        18128,    20,   426,   427,  1151,  6753,    97,  1399,   402,    57,\n",
      "          123,   219,    80,   133,    14,    15,   791,   852,   859,    20,\n",
      "          615,  5703,   330,   859,   198, 28464,   847,    41,  4119,   683,\n",
      "          198, 13244,   847,   384,  1390,    80,   133,    14,    15,   951,\n",
      "          108, 27971,   449,  1049,  5179,   877,    20,   129,  2289,    66,\n",
      "          822, 28811,    39,  4306,  2289,    30,   115, 17242,   145,   198,\n",
      "          462,    86,  1144,    20, 18128,    30,   475,    86,   242,    14,\n",
      "           15,  6488, 13809, 22465,   149, 19385,    20,   425,   885,    30,\n",
      "           91,    12,    80,    39, 23125,    39,  4419,    97,   875,   407,\n",
      "          402,    38,   453,  8144,    11,  2198,    39,   916,    71,   885,\n",
      "          875,    74,  2878, 21811,   813,   849,    66,  6488, 13809, 17256,\n",
      "           96,   645,    71,   926,    97,  1074,   242,    14,   443,   123,\n",
      "          425, 13155,   885,    30,    91,  4915,  1179, 11313,    43, 19754,\n",
      "          402,    38,    39,  5574,    96,  6064,   837,    71,  2322,   572,\n",
      "          264,    97,  5858,    99,   443,    30,   735,    26,   242,    14,\n",
      "           15,   887,  1193,   445,     1,    66, 20617,   811,   172,    41,\n",
      "         1864,    74,  9443,  1038,    46, 13851,   256,    74,    41,  2609,\n",
      "           82,  1802,    43,   885,    30,     1,   827,   264,   806,  2974,\n",
      "          145,   198,   926,    30, 19281,  2181,    41,  6181,    97,  6488,\n",
      "        13809,    30,  6181,   407,    99,  1957,    97,  1603,   242,    14,\n",
      "           15,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'start': 328, 'end': 332, 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = utils.TokenizerWrapperDataset(train_dataset, tokenizer)\n",
    "eval_dataset = utils.TokenizerWrapperDataset(eval_dataset, tokenizer)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 943])\n",
      "tensor([[    2,   145,    30,  ...,     0,     0,     0],\n",
      "        [    2,   344,    32,  ...,     0,     0,     0],\n",
      "        [    2,  3331, 23956,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   507,  7304,  ...,     0,     0,     0],\n",
      "        [    2,  8818,   483,  ...,     0,     0,     0],\n",
      "        [    2,  6089,   210,  ...,     0,     0,     0]])\n",
      "['guid', 'context', 'question', 'position', 'input_ids', 'token_type_ids', 'start', 'end', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "collator = utils.Collator(tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator, num_workers=4)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False, collate_fn=collator, num_workers=4)\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['input_ids'])\n",
    "print(list(batch.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(36311, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (start_linear): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (end_linear): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig(\n",
    "     vocab_size=tokenizer.vocab_size,\n",
    "     max_position_embeddings=1024,\n",
    "     hidden_size=512,\n",
    "     num_hidden_layers=6,\n",
    "     num_attention_heads=8,\n",
    "     intermediate_size=2048\n",
    ")\n",
    "model = models.BertForQuestionAnswering(config)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='review')\n",
    "os.makedirs('dump', exist_ok=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 31):\n",
    "    print(\"Epoch\", epoch)\n",
    "    for batch in tqdm(train_loader, desc='Train'):\n",
    "        del batch['guid'], batch['context'], batch['question'], batch['position']\n",
    "        batch = {key: value.cuda() for key, value in batch.items()}\n",
    "        start = batch.pop('start')\n",
    "        end = batch.pop('end')\n",
    "        \n",
    "        start_logits, end_logits = model(**batch)\n",
    "        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.)\n",
    "        optimizer.step()\n",
    "\n",
    "        step += 1\n",
    "        writer.add_scalar('Train Loss', loss.item(), step)\n",
    "        del batch, start, end, start_logits, end_logits, loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluation\"):\n",
    "            del batch['guid'], batch['context'], batch['question'], batch['position']\n",
    "            batch = {key: value.cuda() for key, value in batch.items()}\n",
    "            start = batch.pop('start')\n",
    "            end = batch.pop('end')\n",
    "\n",
    "            start_logits, end_logits = model(**batch)\n",
    "            loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            del batch, start, end, start_logits, end_logits, loss\n",
    "        loss = sum(losses) / len(losses)\n",
    "        writer.add_scalar('Eval Loss', loss, step)\n",
    "\n",
    "    model.save_pretrained(f'dump/model.{epoch}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnswering(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(36311, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (start_linear): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (end_linear): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.BertForQuestionAnswering.from_pretrained('dump/model.30')\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------1------\n",
      "Context: 지난 11월 14일 토요일, 클룩(KLOOK)은 진에어와 함께 관광 비행 및 내년 홍콩 왕복 항공권을 증정하는 ‘미리 즐기는 홍콩원정대’ 항공여행을 진행했다. 클룩은 본 여행을 위해, 진에어 전세기 운항을 주도해 실현시켰다. 특히 이번 상품은 홍콩 여행 테마를 주제로 잃어버린 여행의 설렘을 다시 느낄 수 있도록 한 관광 비행과 얼리버드 홍콩 왕복 항공권을 결합시켜 예약이 조기 매진되는 등 화제를 불러일으킨 패키지다. 오랜만의 비행을 맞이해, 클룩과 진에어 외에도 유관 기관의 협조가 빛을 발했다. 인천공항에서는 미니 콘서트와 국내선 시설 지원을, 제주공항 관제소에서는 한라산 및 제주도 전역 저공비행 허가를 밀어줬다. 여기에 더해, 홍콩관광청에서도 홍콩 여행을 필수적인 가이드북과 컬러링북 등 특전들이 참가자들에게 제공했다. 홍콩원정대 이름에 걸맞게 홍콩 명물 제니쿠키 역시 서비스되었다. 이처럼 기업과 기관이 협조해 진행한 덕분에, 저렴한 비용에 역대급 특전 제공이 가능했다. 홍콩 테마의 관광 비행 1인과 홍콩 왕복 항공권 1매, 클룩의 홍콩 여행 상품권 10만원권을 포함한 가격이 26만 9천원, 관광 비행 1인과 홍콩 왕복 항공권 2매, 클룩 홍콩 여행 상품권 15만원 포함 패키지는 39만 9천원으로 제공되었다. 관광 비행만 원하는 경우를 위해서는 클룩의 국내 여행 상품권 5만원권을 포함한 16만 9천원의 패키지가 선택 가능했다. 함께 제공되는 클룩 여행상품권은 현지 어트랙션이나 입장권은 물론 호텔과 렌터카 등 다양한 여행상품을 구매할 수 있다. 또한 홍콩 왕복 항공권과 클룩 상품권의 경우 2022년 3월까지의 넉넉한 유효기간으로 제공되어, 코로나19로 유효기간 내 사용이 불가능한 경우 유효기간 연장이 가능하다. 단, 홍콩 항공권은 사용 당시 유류할증료와 공항세만 별도 지불하면 된다. 홍콩원정대 전세기는 오후 3시, 인천국제공항을 이륙해 광주, 제주, 부산, 대구 등을 하늘에서 둘러본 뒤 다시 인천공항으로 돌아왔다. 약 2시간의 비행을 통해, 참가자들은 항공기로 오가던 여행의 추억을 되새겼다. 비행 중 진에어에서는 중화덮밥을 기내식으로 제공했으며, 승무원 엔터테인먼트 팀 ‘랄라진스’의 기내 콘서트와 김포-부산 항공권을 건 경품 이벤트도 성황리에 진행되었다. 한편, 모든 관광 비행은 코로나19 방역 지침을 준수하며 진행되었다. 특히 기내 좌석은 실제 탑승 가능 좌석인 189석의 70%인 132석만 운영되었다. 더불어 탑승객 중 가장 어린 승객은 6살, 최고령자는 84살이라는 이색적인 기록도 남은 비행이기도 했다. 클룩 이준호 한국 지사장은 “여행이 다시 시작되는 설레는 순간을 경험할 수 있도록 하늘에서 즐기는 특별한 여행 경험과 홍콩 왕복 항공권, 클룩 상품권까지 풍성한 혜택을 준비했다”며 “잃어버린 여행이 다시 일상 속 즐거움으로 돌아올 수 있는 그날까지 국내외 다양한 체험 상품들을 선보일 것”이라고 말했다. 또한 향후 다른 FSC, LCC 항공사와의 협업과 관광업계 및 유관기관과의 협조를 바탕으로 여행이 그리운 고객들에게 실질적인 혜택이 돌아갈 이벤트의 주선을 클룩이 주선할 것임을 약속했다.\n",
      "Question: 패키지 중 가장 싼 것은 얼마인가요?\n",
      "Answer: 16만 9천원\n",
      "------2------\n",
      "Context: 창\n",
      ":초기장비. 랜스를 투척해 공격한다. 특징이 없는 가장 표준적인 무기로 수평으로 날아간다. 동시에 2발을 쏠 수 있다. 단검에 비해 약간 공격판정이 크지만 날아가는 속도는 느리다.\n",
      ":마법은 『번개』. 머리위에 낙뢰가 떨어져 좌우로 방출된다. 발생시간은 짧지만 위력이 강해 발생중에 계속 적에게 데미지를 줄 수 있다.(단 히트경직이 있어서 기본적으론 1~2발이 한계이다.) 맞는 방향에 따라 보스를 한방에 죽일 수도 있다. 주인공의 머리 위로 떨어지는 번개는 보스도 한 방에 죽일 수 있다.\n",
      "단검\n",
      ":작은 단검을 던진다. 동시에 3발을 쏠 수 있고 비행속도가 빠른 반면 공격판정이 작다. 모든 무기 중 가장 쓰기 쉽다.\n",
      ":마법은 『분신』. 아더의 움직임을 따라하는 분신이 일정시간 나타나 공격력이 2배로 증가한다. 본체와는 약간의 타임갭이 있다.\n",
      "\n",
      "횃불\n",
      ":푸른 불공을 포물선으로 던진다. 땅에 닿으면 불기둥이 일어나 앞으로 나가며 적을 공격한다. 동시에 2발을 쏠 수 있다. 발생 중엔 공격판정이 계속 되지만 판정범위는 크지않고 히트경직 관계 상, 큰 데미지는 줄 수 없다. 보기와는 다르게 쓰기어려운 무기 중 하나이다.\n",
      ":마법은 『화구』. 두 가지 불덩어리가 큰 회전을 하며 좌우의 위로 비스듬히 날아간다. 공격력은 보통이지만 너무 막 쓰면 좋지 않다.\n",
      "\n",
      "이카루스의 방패\n",
      ":녹색원반을 프리스비처럼 던진다. 서서 던지면 직선으로 날아가지만 앉아서 쏘면 지면을 따라 날아간다. 동시에 2발 쏠 수 있다. 비행속도는 창보다 빠르고 단검보다 느리다.\n",
      ":마법은 『거울』. 아더의 앞에 거울이 놓이며 적의 총알을 막을 수 있다. 적에게 맞출 수도 있지만 범위가 좁기 때문에 공격에는 좋지 않다. 내구력 있는 적의 경우 접촉하면 거울이 부서진다.\n",
      "\n",
      "도끼\n",
      ":상방 30도로 날아가며 적을 관통한다. 원칙적으로 동시에 한발 밖에 쏠 수 없다. 파괴력은 있지만 정면에 있는 적에겐 쓰지 어렵기 때문에 잘못 먹으면 상황에 따라 게임오버될 수 도 있다.\n",
      ":마법은 『폭렬』. 아더 주위에 거대한 폭발이 일어나 데미지를 준다. 폭발의 범위는 보기에 비해 좁고 공격력은 높지만 일격필살이라고는 할 수 없기 때문에 용도는 그다지.\n",
      "\n",
      "장검\n",
      ":손에든 검을 휘두른다. 모든 무기중에서 유일한 근거리 무기이며 창보다 2배의 공격력과 연사가 가능하지만 사거리가 극히 짧다.\n",
      ":마법은 『번개용』. 번개 모양의 용이 화면을 돈 후에 아더에게 돌아온다. 발생 중에 틈이 너무 크기 때문에 사용하기 매우 어렵다.\n",
      ":스테이지4의 보스에선 약점까지 안닿기 때문에 등장하지 않는다.\n",
      ":전의 스테이지에서 획득한 이후 무기가 변경되지 않았을 경우 오직 마법으로만 스테이지4를 클리어해야 한다.\n",
      "\n",
      "사이코캐논\n",
      ":대천사 미카엘의 축복을 받은 최강의 무기. 2회차에서만 등장. 황금갑옷을 입고 있을 때 보물상자에서 나오며 동시 2발 쏠 수 있다. 사정거리는 약간 짧지만 파괴력이나 공격판정면에서 뛰어나다.\n",
      ":마법은 없지만 황금갑옷을 입은 상태에선 사정거리와 속도가 올라가고 적의 총알을 상쇄시킬 수 있다. 졸개들에 대해선 관통력도 가진다. 이 무기를 착용하지 않으면 최종보스 루시퍼와 싸울 수 없다.\n",
      ":갑옷의 상태에 따라 사정거리가 달라진다. 팬티만 입은 상태에서는 장검과 동일하지만 황금갑옷을 입은 상태일 경우 화면의 절반까지 날아가며 철갑옷의 경우 그 둘의 중간 정도 된다.\n",
      "Question: 동시에 2발을 쏠 수 있는 무기 중 포물선 형태로 공격하는 것은 무엇인가?\n",
      "Answer: 횃불\n",
      "------3------\n",
      "Context: 넷기어코리아(지사장 김진겸, 이하 넷기어)가 세종문화회관에서 열리는 ‘칸딘스키 미디어아트 & 음악을 그리는 사람들’展에 뮤럴 디지털 캔버스를 지원•전시한다. 1월 10일에서 3월 9일까지 두 달 동안 세종문화회관 미술관 1~2관에서 열리는 이번 전시는 ‘현대 추상의 아버지’라 불리는 바실리 칸딘스키(Wassily Kandinsky, 1866-1944)를 중심으로 열린다. 미술과 음악의 융합을 끊임없이 추구하였던 칸딘스키의 예술 이론을 2020년에 적용, 미디어아트와 음악이 결합된 것이 특징이다. 뮤럴 캔버스는 ‘칸딘스키와 뉴미디어’를 주제로 하는 1관에 전시된다. 10여대의 뮤럴 캔버스를 통해 칸딘스키의 대표작들을 선보일 예정이다. 원본에서 느낄 수 있는 칸딘스키 작품의 질감과 붓터치를 생생하게 감상할 수 있음과 동시에 시간의 흐름에 따른 칸딘스키 화풍의 변화를 확인할 수 있다. 이번 전시를 기획한 글로벌교육콘텐츠의 정수경 큐레이터는 “뮤럴 캔버스가 상당수의 칸딘스키 작품을 보유하고 있음은 물론, 원화의 질감을 높은 퀄리티로 재현한다는 점에서 화풍의 변화 등을 보여주는 데 있어 최적이라고 판단해 이번 전시에 함께하게 됐다”고 밝혔다. 또한 “유명한 작품 뿐만이 아니라 1907년 칸딘스키가 참여한 파리의 살롱 전시 카탈로그의 목판화 <Night> 등의 아카이브 작품을 계속해서 추가하고 있다는 점에서 뮤럴이 예술가들의 작품을 아카이빙할 수 있는 새로운 대안이 될 수 있지 않을까 생각한다”고 말했다. 전시관 옆에 마련된 아트샵에서도 뮤럴 디지털 캔버스를 체험•구매할 수 있다. 넷기어는 전시관 로비와 아트샵 내에 뮤럴 캔버스 27인치와 21.5인치를 설치해 고흐, 모네, 클림트, 드가, 세잔 등 전 세계 3만여 점의 명화를 직접 체험하고 감상할 수 있는 공간을 마련했다. 김희준 넷기어코리아 마케팅 담당 이사는 “100년의 시간을 뛰어 넘어 칸딘스키와 소통할 수 있는 자리에 뮤럴 캔버스가 함께할 수 있어 기쁘게 생각한다”며 “이번 전시회를 시작으로 뮤럴 디지털 캔버스를 체험할 수 있는 공간을 다양하게 늘려갈 예정”이라고 말했다.\n",
      "Question: 뮤럴이 예술가들의 작품을 아카이빙할 수 있는 대안이 될 수 있다고 말한 사람의 이름은?\n",
      "Answer: 정수경\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    }
   ],
   "source": [
    "for idx, raw_sample in zip(range(1, 4), train_dataset):\n",
    "    sample = dict(raw_sample) \n",
    "    context = sample.pop('context')\n",
    "    question = sample.pop('question')\n",
    "    position = sample.pop('position')\n",
    "    start, end = sample.pop('start'), sample.pop('end')\n",
    "    del sample['guid']\n",
    "\n",
    "    sample = {key: value.cuda()[None, :] for key, value in sample.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = model(**sample)\n",
    "    start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
    "\n",
    "    print(f'------{idx}------')\n",
    "    print('Context:', context)\n",
    "    print('Question:', question)\n",
    "    print('Answer:', tokenizer.logits2answer(raw_sample, start_logits, end_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Questions 4008\n",
      "{'guid': 'd14cb73158624cf094c546d856fd3c80', 'context': 'BMW 코리아(대표 한상윤)는 창립 25주년을 기념하는 ‘BMW 코리아 25주년 에디션’을 한정 출시한다고 밝혔다. 이번 BMW 코리아 25주년 에디션(이하 25주년 에디션)은 BMW 3시리즈와 5시리즈, 7시리즈, 8시리즈 총 4종, 6개 모델로 출시되며, BMW 클래식 모델들로 선보인 바 있는 헤리티지 컬러가 차체에 적용돼 레트로한 느낌과 신구의 조화가 어우러진 차별화된 매력을 자랑한다. 먼저 뉴 320i 및 뉴 320d 25주년 에디션은 트림에 따라 옥스포드 그린(50대 한정) 또는 마카오 블루(50대 한정) 컬러가 적용된다. 럭셔리 라인에 적용되는 옥스포드 그린은 지난 1999년 3세대 3시리즈를 통해 처음 선보인 색상으로 짙은 녹색과 풍부한 펄이 오묘한 조화를 이루는 것이 특징이다. M 스포츠 패키지 트림에 적용되는 마카오 블루는 1988년 2세대 3시리즈를 통해 처음 선보인 바 있으며, 보랏빛 감도는 컬러감이 매력이다. 뉴 520d 25주년 에디션(25대 한정)은 프로즌 브릴리언트 화이트 컬러로 출시된다. BMW가 2011년에 처음 선보인 프로즌 브릴리언트 화이트는 한층 더 환하고 깊은 색감을 자랑하며, 특히 표면을 무광으로 마감해 특별함을 더했다. 뉴 530i 25주년 에디션(25대 한정)은 뉴 3시리즈 25주년 에디션에도 적용된 마카오 블루 컬러가 조합된다. 뉴 740Li 25주년 에디션(7대 한정)에는 말라카이트 그린 다크 색상이 적용된다. 잔잔하면서도 오묘한 깊은 녹색을 발산하는 말라카이트 그린 다크는 장식재로 활용되는 광물 말라카이트에서 유래됐다. 뉴 840i xDrive 그란쿠페 25주년 에디션(8대 한정)은 인도양의 맑고 투명한 에메랄드 빛을 연상케 하는 몰디브 블루 컬러로 출시된다. 특히 몰디브 블루는 지난 1993년 1세대 8시리즈에 처음으로 적용되었던 만큼 이를 오마주하는 의미를 담고 있다.', 'question': '말라카이트에서 나온 색깔을 사용한 에디션은?', 'position': [(0, 3), (4, 7), (7, 8), (8, 10), (11, 14), (14, 15), (15, 16), (17, 19), (20, 22), (22, 24), (24, 25), (26, 28), (28, 29), (29, 30), (31, 32), (32, 35), (36, 39), (40, 42), (42, 44), (45, 48), (48, 49), (49, 50), (51, 53), (54, 56), (56, 59), (60, 62), (62, 63), (63, 64), (65, 67), (68, 71), (72, 75), (76, 78), (78, 80), (81, 84), (84, 85), (85, 87), (88, 90), (90, 92), (93, 96), (96, 97), (97, 98), (99, 102), (103, 104), (104, 107), (107, 108), (109, 110), (110, 113), (113, 114), (115, 116), (116, 119), (119, 120), (121, 122), (122, 125), (126, 127), (128, 129), (129, 130), (130, 131), (132, 133), (133, 134), (135, 137), (137, 138), (139, 141), (141, 142), (142, 143), (143, 144), (145, 148), (149, 152), (153, 155), (155, 156), (156, 157), (158, 161), (162, 163), (164, 165), (165, 166), (167, 171), (172, 174), (174, 175), (176, 178), (178, 179), (180, 182), (182, 183), (184, 186), (186, 187), (187, 188), (189, 191), (191, 192), (193, 195), (195, 196), (197, 199), (199, 200), (201, 205), (206, 208), (208, 209), (209, 210), (211, 213), (213, 214), (215, 217), (217, 219), (219, 220), (221, 223), (224, 225), (226, 229), (229, 230), (231, 232), (233, 234), (235, 238), (238, 239), (240, 242), (242, 244), (245, 248), (248, 249), (250, 252), (252, 253), (254, 256), (257, 261), (262, 264), (264, 265), (265, 267), (267, 268), (269, 271), (271, 272), (273, 275), (276, 279), (280, 282), (282, 283), (283, 285), (285, 286), (287, 289), (289, 290), (291, 293), (293, 294), (295, 297), (297, 299), (299, 300), (301, 304), (305, 307), (307, 308), (309, 311), (311, 312), (312, 313), (314, 318), (319, 321), (321, 322), (323, 325), (326, 330), (330, 331), (332, 333), (333, 335), (336, 337), (337, 340), (340, 341), (342, 344), (345, 347), (348, 351), (352, 354), (354, 356), (357, 358), (358, 359), (360, 362), (362, 363), (364, 366), (366, 367), (368, 370), (371, 373), (373, 374), (375, 377), (377, 378), (379, 381), (381, 382), (383, 384), (384, 385), (386, 388), (388, 389), (389, 390), (390, 391), (392, 393), (394, 397), (398, 401), (402, 404), (404, 405), (406, 408), (408, 409), (409, 410), (411, 414), (415, 417), (417, 418), (419, 423), (423, 424), (425, 426), (426, 428), (429, 430), (430, 433), (433, 434), (435, 437), (438, 440), (441, 444), (445, 446), (447, 448), (448, 450), (450, 451), (452, 455), (456, 458), (458, 459), (460, 462), (462, 463), (463, 464), (465, 467), (467, 468), (468, 469), (469, 470), (471, 472), (473, 476), (476, 477), (478, 480), (480, 482), (483, 486), (486, 487), (487, 489), (489, 490), (491, 493), (493, 494), (494, 495), (496, 499), (500, 505), (506, 509), (510, 512), (512, 513), (514, 516), (516, 518), (518, 519), (520, 523), (523, 524), (525, 529), (529, 530), (530, 531), (532, 534), (535, 538), (539, 542), (543, 548), (549, 552), (552, 553), (554, 556), (557, 558), (559, 561), (561, 562), (563, 564), (564, 565), (566, 568), (568, 569), (570, 572), (572, 573), (573, 574), (574, 575), (576, 578), (579, 581), (581, 582), (583, 585), (585, 587), (588, 590), (590, 591), (592, 594), (594, 595), (595, 596), (597, 598), (598, 599), (599, 600), (600, 601), (602, 603), (604, 607), (607, 608), (609, 611), (611, 613), (614, 617), (617, 618), (618, 620), (620, 621), (622, 624), (624, 625), (625, 626), (627, 628), (629, 630), (630, 633), (634, 636), (636, 638), (639, 642), (642, 643), (643, 644), (645, 647), (647, 648), (649, 652), (653, 655), (656, 658), (658, 659), (660, 662), (662, 664), (664, 665), (666, 667), (668, 671), (671, 673), (674, 676), (676, 678), (679, 682), (682, 683), (683, 684), (684, 685), (686, 688), (688, 689), (689, 690), (690, 691), (692, 697), (698, 700), (701, 703), (704, 706), (706, 707), (708, 710), (710, 712), (712, 713), (714, 716), (716, 717), (717, 719), (719, 720), (721, 723), (723, 724), (725, 726), (726, 727), (728, 730), (730, 731), (732, 734), (734, 735), (735, 736), (737, 742), (743, 745), (746, 748), (748, 749), (750, 752), (752, 754), (755, 757), (757, 758), (758, 759), (760, 762), (763, 768), (768, 770), (771, 773), (773, 774), (774, 775), (775, 776), (777, 778), (779, 782), (782, 783), (784, 790), (791, 793), (793, 795), (796, 798), (798, 800), (801, 804), (804, 805), (805, 806), (806, 807), (808, 810), (810, 811), (811, 812), (813, 815), (815, 816), (816, 817), (818, 819), (819, 820), (821, 823), (823, 824), (825, 829), (830, 831), (831, 832), (833, 835), (835, 836), (837, 838), (838, 839), (840, 843), (844, 846), (847, 849), (849, 850), (851, 853), (853, 855), (855, 856), (857, 859), (860, 863), (864, 866), (866, 867), (868, 870), (871, 875), (875, 876), (877, 878), (878, 880), (881, 882), (882, 885), (885, 886), (887, 889), (889, 891), (892, 894), (894, 895), (895, 896), (896, 897), (898, 900), (901, 902), (902, 903), (904, 907), (907, 908), (908, 909), (910, 912), (912, 913), (914, 915), (915, 916), (917, 918), (918, 919), (919, 920)], 'input_ids': tensor([    2,     1,    11,   563,  7182,    97,  2079,    71,  8888,    20,\n",
      "          170,     3,  5820,  3014,   173,  1022, 24687,   175,    39,  3363,\n",
      "          132,  5265,    97,  3310,    57,    39,   183,  5820,  3014,   132,\n",
      "         5265,  8888,   191,    97,  5137,   585,  1564,  1128,    14,    15,\n",
      "          284,  5820,  3014,   132,  5265,  8888,   173,  1296,   132,  5265,\n",
      "         8888,   175,    20,  5820,    65,   538,   172,   246,   538,    66,\n",
      "          483,   538,    66,  1007,   538,   258,   193,  5133,    66,   129,\n",
      "          194,  1910,   142,   585,    80,   123,    66,  5820,  4433,  1910,\n",
      "          264,   142,   915,  2150,    38,    39, 27354,  5145,     7, 16862,\n",
      "           32,  2068,   163, 10989,   142,    71,  4422,    74, 27930,    41,\n",
      "         8393,     7, 17009,  2034,   189,   245,  5931,    97,  5143,    84,\n",
      "           15,  2472,  5462,  3166,  5848,    46,  5462,  3166, 17314,   132,\n",
      "         5265,  8888,    20, 19868,    32,   101, 10881,  4215,   173,   506,\n",
      "          634,  5137,   175,  3413, 17336,  5147,   173,   506,   634,  5137,\n",
      "          175,  5145,     7,  2068,   229,    15, 16850,  7193,    32,  2068,\n",
      "           80,    39, 10881,  4215,    20,   726,  2305,   126,    65,  1491,\n",
      "           65,   538,    82,   212,   467,   915,  4530,    43, 14280,    20,\n",
      "         5702,    74,  5957,    71,     1, 21920,    71,  8393,    82,  1523,\n",
      "           39,    99,    30,   611,    30,    14,    15,  1839,  1370, 12371,\n",
      "        19868,    32,  2068,    80,    39, 17336,  5147,    39,  5278,   126,\n",
      "           63,  1491,    65,   538,    82,   212,   467,   915,  2150,    38,\n",
      "          134,    66,     1, 15938,    39,  5145,  4914,    30,  5931,    30,\n",
      "           14,    15,  5462, 17199, 17314,   132,  5265,  8888,   173,   132,\n",
      "          634,  5137,   175,    20, 27931, 27932,  5337,  5145,   142,   585,\n",
      "          229,    15,  5820,     7,   677,   126,    32,   467,   915, 27931,\n",
      "        27932,  5337,    39,  9422,   840, 10072,   243,  1187,    20, 15774,\n",
      "           97,  5143,    57,   123,    66,   791,  5251,    97,     1,    43,\n",
      "         1504,    93,   916,   361,    97,   840,   242,    14,    15,  5462,\n",
      "        10719,  5848,   132,  5265,  8888,   173,   132,   634,  5137,   175,\n",
      "           20,  5462,    65,   538,   132,  5265,  8888,    32,    86,  2068,\n",
      "          245, 17336,  5147,  5145,     7,  6940,   229,    15,  5462, 19530,\n",
      "         6834,   132,  5265,  8888,   173,   483,   634,  5137,   175,    32,\n",
      "           39,     1,  4215, 21209,  4530,    30,  2068,   229,    15, 27933,\n",
      "           57,    52,    86, 21920,    71,  1187,    20,  5702,    97, 15659,\n",
      "           57,    39,     1,  4215, 21209,    39,  9196,     1,   793,    80,\n",
      "           39,  5264,     1,    11,  5902,    13,    14,    15,  5462, 19371,\n",
      "         5848,     1, 23759, 16942,   132,  5265,  8888,   173,  1007,   634,\n",
      "         5137,   175,    20,   715,  2171,    41, 19966,   243,  6626,    71,\n",
      "        27646,  2800,    97,  7841,  4441,    57,    39, 27934,  5147,  5145,\n",
      "          142,   585,   229,    15,   791, 27934,  5147,    39,   726,  1302,\n",
      "          126,   285,  1491,  1007,   538,    32,   467,    43,  2068,    80,\n",
      "          133,   553,  1297,    30,    82,     1,    57,    39,  1234,    82,\n",
      "         2132,   243,    38,    14,    15,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'start': None, 'end': None, 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.TokenizedKoMRC.load('data/test.json')\n",
    "test_dataset = utils.TokenizerWrapperDataset(test_dataset, tokenizer)\n",
    "print(\"Number of Questions\", len(test_dataset))\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 4008/4008 [00:45<00:00, 88.38it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "os.makedirs('out', exist_ok=True)\n",
    "with torch.no_grad(), open('out/baseline.csv', 'w') as fd:\n",
    "    writer = csv.writer(fd)\n",
    "    writer.writerow(['Id', 'Predicted'])\n",
    "\n",
    "    rows = []\n",
    "    for raw_sample in tqdm(test_dataset, \"Testing\"):\n",
    "        sample = dict(raw_sample) \n",
    "        guid = sample.pop('guid')\n",
    "        context = sample.pop('context')\n",
    "        position = sample.pop('position')\n",
    "        start, end = sample.pop('start'), sample.pop('end')\n",
    "        del sample['question']\n",
    "\n",
    "        sample = {key: value.cuda()[None, :] for key, value in sample.items()}\n",
    "        start_logits, end_logits = model(**sample)\n",
    "        start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
    "\n",
    "        rows.append([guid, tokenizer.logits2answer(raw_sample, start_logits, end_logits)])\n",
    "    \n",
    "    writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
